{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repo Health Visualization Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"vaadin\" # Change to user name of your GitHub repository\n",
    "repository_name = \"flow\" # Change to repository name of your GitHub repository\n",
    "\n",
    "input_json_file = r'' # Add path to the .json file containing all issue and pull request data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In which formats to export plot data\n",
    "EXPORT_FORMAT = \"both\" # \"csv\", \"json\" or \"both\"\n",
    "# If plots created using Matplotlib should be exported as .pngs\n",
    "EXPORT_PNGS = True # True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github, RateLimitExceededException # GNU licence - LGPL-3.0 and GPL-3.0 () # conda install PyGithub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from transformers import AutoModel # What does this do?\n",
    "from functools import partial\n",
    "import sklearn \n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from langdetect import detect\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# cuda_available = torch.cuda.is_available()\n",
    "# print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "# # Get the name of the GPU\n",
    "# if cuda_available:\n",
    "#     gpu_name = torch.cuda.get_device_name(0)\n",
    "#     print(f\"GPU: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The microlevel metrics focus on the individual system components (programs, modules, and procedures) and require a detailed knowledge of their internal mechanisms. Examples of microlevel metrics are McCabe’s cyclomatic complexity metric [5] and Halstead’s software science measurements [6]. Macrolevel metrics, on the other hand, focus on the interconnection of the system components. Examples of these metrics include Myers qualitative coupling evaluation [4], Yin and Winchester’s interlevel measurements [7], the entropy measures of Channon [8], and the more recent technique of Henry and Kafura based on information flow [9,10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_pat_file = r\"D:\\Code\\Github\\private\\pat.txt\"\n",
    "# input_pat_file = r\"D:\\Github\\private\\pat.txt\"\n",
    "\n",
    "# input_json_file = r'D:\\Code\\Github\\SmartDelta\\repo_data.json'\n",
    "# input_json_file = r'D:\\Code\\Github\\SmartDelta\\repo_data_partial_20240607_103551.json'\n",
    "# input_json_file = r'D:\\Code\\Github\\SmartDelta\\repo_data_partial_20240611_095503.json'\n",
    "# input_json_file = r'D:\\Code\\Github\\SmartDelta\\repo_data_partial_20240702_111311.json'\n",
    "\n",
    "# all files except the one below have reactions\n",
    "# input_json_file = r'D:\\Code\\Github\\SmartDelta\\repo_data_partial_20240702_131051.json'\n",
    "# input_json_file = r'D:\\Github\\Backups\\SmartDelta\\repo_data_partial_20240702_131051.json'\n",
    "# input_json_file = r'D:\\GitHub\\SmartDelta\\data\\vaadin_flow_data.json'\n",
    "\n",
    "\n",
    "# input_json_file = r'D:\\Github\\SmartDelta\\repo_data.json'\n",
    "# input_json_file = r'D:\\GitHub\\SmartDelta\\repo_data_partial_20240612_090858.json'\n",
    "\n",
    "# user_name = \"vaadin\"\n",
    "# repository_name = \"flow\"\n",
    "\n",
    "NUMBER = \"number\"\n",
    "\n",
    "# Initialize data structures\n",
    "issues_data = []\n",
    "pull_requests_data = []\n",
    "issue_pr_map = {}\n",
    "\n",
    "amount_of_issues = 0\n",
    "amount_of_pull_requests = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Github API with a PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GITHUB_PAT from .env file\n",
    "GITHUB_PAT = os.getenv(\"GITHUB_PAT\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(input_pat_file) as file:\n",
    "#     GITHUB_PAT = file.read().strip()\n",
    "github_api = Github(GITHUB_PAT)\n",
    "\n",
    "repo = github_api.get_repo(f\"{user_name}/{repository_name}\")\n",
    "\n",
    "repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to fetch issue and pull request data from Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all data to a .json file\n",
    "def save_data(partial=False):\n",
    "    filename = 'repo_data.json'\n",
    "    if partial:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'repo_data_partial_{timestamp}.json'\n",
    "    data = {\n",
    "        \"issues\": issues_data,\n",
    "        \"pull_requests\": pull_requests_data,\n",
    "        \"issue_pr_map\": issue_pr_map\n",
    "    }\n",
    "    with open(filename, 'w') as output_file:\n",
    "        json.dump(data, output_file, indent=4)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Map issues to PRs and save the data to issue_pr_map\n",
    "def map_issues_to_prs():\n",
    "    for pr in pull_requests_data:\n",
    "        if pr['title']:\n",
    "            issue_numbers = re.findall(r'#(\\d+)', pr['title'])\n",
    "            for issue_number in issue_numbers:\n",
    "                if issue_number not in issue_pr_map: # Create a new list if not present\n",
    "                    issue_pr_map[issue_number] = []\n",
    "                issue_pr_map[issue_number].append(pr['number'])\n",
    "        if pr['description']:\n",
    "            issue_numbers = re.findall(r'#(\\d+)', pr['description'])\n",
    "            for issue_number in issue_numbers:\n",
    "                if issue_number not in issue_pr_map: # Create a new list if not present\n",
    "                    issue_pr_map[issue_number] = []\n",
    "                issue_pr_map[issue_number].append(pr['number'])\n",
    "\n",
    "\n",
    "# # Fetch reactions count for an object\n",
    "# def get_reactions_count(obj):\n",
    "#     try:\n",
    "#         reactions = obj.get_reactions()\n",
    "#         return sum(1 for _ in reactions)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching reactions for {obj}: {e}\")\n",
    "#         return -1\n",
    "\n",
    "# Fetch all issues\n",
    "def fetch_issues(issues_data = []):\n",
    "    # print(len(issues_data))\n",
    "    # return\n",
    "    # Fetch all issues\n",
    "    try:\n",
    "        issues = repo.get_issues(state=\"all\")\n",
    "        # issues = repo.get_issues(state=\"all\").reversed # TODO: TEST\n",
    "        print(len(list(issues)))\n",
    "        amount_of_issues = len(list(issues)) # TODO: TEST\n",
    "        print(amount_of_issues)\n",
    "        latest_issue = 0\n",
    "        # try:\n",
    "        #     issues_data\n",
    "        # except NameError:\n",
    "        #     issues_data = []\n",
    "        #     print(\"issues_data wasn't defined when fetch_issues() got called\")\n",
    "        # else:\n",
    "        #     issues_data = []\n",
    "        #     print(\"issues_data wasn't defined when fetch_issues() got called\")\n",
    "\n",
    "        if len(issues_data) < 1: pass # No issue data found\n",
    "        elif isinstance(issues_data, dict): # Dict of dicts\n",
    "            latest_issue = max(issue[NUMBER] for issue in issues_data.values())\n",
    "        elif isinstance(issues_data, list): # List of dicts\n",
    "            latest_issue = max(issue[NUMBER] for issue in issues_data)\n",
    "        else:\n",
    "            raise TypeError(\"issues_data must be either a dict or a list.\")\n",
    "            \n",
    "        # print(f\"Amount of issues: {amount_of_issues} | Issue Counter: {issue_counter}\")\n",
    "        # print(f\"Should fetch {amount_of_issues - issue_counter} issues\\n\")\n",
    "\n",
    "        print(f\"Amount of issues: {amount_of_issues} | Length of issue data: {len(issues_data)} | Latest issue: {latest_issue}\")\n",
    "        print(f\"Should fetch approximately {amount_of_issues - len(issues_data)} issues\\n\")\n",
    "    except RateLimitExceededException as e:\n",
    "        print(f\"\\n{e}\\n\")\n",
    "        rate_limit_exceed_handling()\n",
    "    \n",
    "    try:\n",
    "        for i, issue in enumerate(issues):\n",
    "            # if(i < issue_counter): continue # Skip already processed issues BUG: New issues seem to be added to the start of the list not the end\n",
    "            # if(amount_of_issues - i < issue_counter): continue # Skip already processed issues TODO: TEST!\n",
    "            if issue.number < latest_issue: continue\n",
    "        \n",
    "            issues_data.append({\n",
    "                \"number\": issue.number,\n",
    "                \"title\": issue.title,\n",
    "                \"state\": issue.state,\n",
    "                \"created_at\": issue.created_at.isoformat(),\n",
    "                \"closed_at\": issue.closed_at.isoformat() if issue.closed_at else None,\n",
    "                \"user\": issue.user.login,\n",
    "                \"comments\": issue.comments,\n",
    "                \"description\": issue.body,\n",
    "                \"description_length\": len(issue.body) if issue.body else 0,\n",
    "                \"labels\": [label.name for label in issue.labels],\n",
    "                \"issue_author_association\": issue.raw_data.get(\"author_association\", \"NONE\")#,\n",
    "                # \"reactions\": get_reactions_count(issue)\n",
    "            })\n",
    "    except RateLimitExceededException as e:\n",
    "        print(f\"\\n{e}\\n\")\n",
    "        rate_limit_exceed_handling()\n",
    "        fetch_issues()\n",
    "\n",
    "# Fetch all pull requests\n",
    "def fetch_pull_requests(pull_requests_data = []):\n",
    "    # Fetch all pull requests\n",
    "    try:\n",
    "        prs = repo.get_pulls(state=\"all\")\n",
    "        # prs = repo.get_pulls(state=\"all\").reversed # TODO: TEST\n",
    "        amount_of_pull_requests = len(list(prs)) # TODO: TEST\n",
    "\n",
    "        latest_pr = 0\n",
    "        # try:\n",
    "        #     pull_requests_data\n",
    "        # except NameError:\n",
    "        #     pull_requests_data = []\n",
    "        #     print(\"pull_requests_data wasn't defined when fetch_pull_requests() got called\")\n",
    "        # else:\n",
    "        #     pull_requests_data = []\n",
    "        #     print(\"pull_requests_data wasn't defined when fetch_pull_requests() got called\")\n",
    "\n",
    "        if len(pull_requests_data) < 1: pass # No issue data found\n",
    "        elif isinstance(pull_requests_data, dict): # Dict of dicts\n",
    "            latest_pr = max(issue[NUMBER] for issue in pull_requests_data.values())\n",
    "        elif isinstance(pull_requests_data, list): # List of dicts\n",
    "            latest_pr = max(issue[NUMBER] for issue in pull_requests_data)\n",
    "        else:\n",
    "            raise TypeError(\"pull_requests_data must be either a dict or a list.\")\n",
    "\n",
    "        # print(f\"Amount of pull requests: {amount_of_pull_requests} | PR Counter: {pr_counter}\")\n",
    "        # print(f\"Should fetch {amount_of_pull_requests - pr_counter} PRs\\n\")\n",
    "        print(f\"Amount of pull requests: {amount_of_pull_requests} | Length of pull requests: {len(pull_request_data)} | Latest pull request: {latest_pr}\")\n",
    "        print(f\"Should fetch approximately {amount_of_pull_requests - len(pull_request_data)} pull requests\\n\")\n",
    "    except RateLimitExceededException as e:\n",
    "        print(f\"\\n{e}\\n\")\n",
    "        rate_limit_exceed_handling()\n",
    "    try:\n",
    "        for i, pr in enumerate(prs):\n",
    "            # if(i < pr_counter): continue # Skip already processed PRs\n",
    "            # if(amount_of_pull_requests - i < pr_counter): continue # Skip already processed PRs TODO: TEST!\n",
    "            if pr.number < latest_pr: continue\n",
    "\n",
    "            pr_files = pr.get_files()\n",
    "            files_changed = [{\"filename\": f.filename, \"additions\": f.additions, \"deletions\": f.deletions} for f in pr_files]\n",
    "            pr_commits = pr.get_commits()\n",
    "            commit_data = [{\n",
    "                \"sha\": commit.sha,\n",
    "                \"author\": commit.commit.author.name,\n",
    "                \"date\": commit.commit.author.date.isoformat(),\n",
    "                \"message\": commit.commit.message\n",
    "            } for commit in pr_commits]\n",
    "            pull_requests_data.append({\n",
    "                \"number\": pr.number,\n",
    "                \"title\": pr.title,\n",
    "                \"state\": pr.state,\n",
    "                \"created_at\": pr.created_at.isoformat(),\n",
    "                \"merged_at\": pr.merged_at.isoformat() if pr.merged_at else None,\n",
    "                \"closed_at\": pr.closed_at.isoformat() if pr.closed_at else None,\n",
    "                \"user\": pr.user.login,\n",
    "                \"comments\": pr.comments,\n",
    "                \"review_comments\": pr.review_comments,\n",
    "                \"description\": pr.body,\n",
    "                \"description_length\": len(pr.body) if pr.body else 0,\n",
    "                \"additions\": pr.additions,\n",
    "                \"deletions\": pr.deletions,\n",
    "                \"changed_files\": pr.changed_files,\n",
    "                \"files\": files_changed,\n",
    "                \"commits\": commit_data,\n",
    "                \"labels\": [label.name for label in pr.labels],\n",
    "                \"mergeable_state\": pr.mergeable_state\n",
    "            })\n",
    "    except RateLimitExceededException as e:\n",
    "        print(f\"\\n{e}\\n\")\n",
    "        rate_limit_exceed_handling()\n",
    "        fetch_pull_requests(i)\n",
    "\n",
    "# handle rate limit exceed exceptions\n",
    "# Alternative solution would be to keep track of rate limits: https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api\n",
    "def rate_limit_exceed_handling():\n",
    "    print(\"Rate limit exceeded again. Saving partial data...\")\n",
    "    save_data(partial=True)\n",
    "    print(\"Partial data saved. Waiting until reset...\")\n",
    "    reset_time = github_api.rate_limiting_resettime\n",
    "    sleep_time = max(0, reset_time - time.time() + 10)  # Adding a buffer of 10 seconds\n",
    "    print(f\"Sleeping for {sleep_time:.0f} seconds\")\n",
    "    time.sleep(sleep_time)\n",
    "    # Retry after sleep\n",
    "    print(\"\\nRetrying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how much longer to wait before next request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_time = github_api.rate_limiting_resettime\n",
    "sleep_time = max(0, reset_time - time.time() + 10)  # Adding a buffer of 10 seconds\n",
    "print(f\"Sleeping for {sleep_time:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def remove_duplicates(data, data_type):\n",
    "    if(data_type == \"issue\"):\n",
    "        if(len(data) == amount_of_issues):\n",
    "            print(\"No duplicates to remove\")\n",
    "            return\n",
    "        \n",
    "    if(data_type == \"pull request\"):\n",
    "        if(len(data) == amount_of_pull_requests):\n",
    "            print(\"No duplicates to remove\")\n",
    "            return\n",
    "\n",
    "    print(f\"Number of {data_type}s before duplicate removal: {len(data)}\")\n",
    "    print(f\"Removing duplicates {data_type}s ...\")\n",
    "    unique_entries = list(set([entry['number'] for entry in data]))\n",
    "    print(f\"Number of unique {data_type}s: {len(unique_entries)}\")\n",
    "    print(f\"Number of duplicate {data_type}s: {len(data) - len(unique_entries)}\")\n",
    "\n",
    "    # Track issue numbers and their occurrences\n",
    "    data_tracker = defaultdict(list)\n",
    "\n",
    "    for entry in data_tracker:\n",
    "        data_tracker[entry['number']].append(entry)\n",
    "\n",
    "    # Identify duplicates\n",
    "    duplicates = {number: entry_data for number, entry_data in data_tracker.items() if len(entry_data) > 1}\n",
    "\n",
    "    # Print duplicates\n",
    "    print(f\"Duplicates: {list(duplicates.keys())}\")\n",
    "\n",
    "    # If there are duplicates, print the detailed information of these issues\n",
    "    if duplicates:\n",
    "        print(f\"\\nDetails of duplicate {data_type}s:\")\n",
    "        for number, data_entries in duplicates.items():\n",
    "            print(f\"{data_type[0].upper()}{data_type[1:]} number: {number}\")\n",
    "            for entry in data_entries:\n",
    "                print(entry) # Print the detailed information of the issue - only thing that was different so far was the reactions value for issues\n",
    "                # TODO: only keep 1 of them\n",
    "                del entry\n",
    "\n",
    "    print(f\"Done removing duplicate {data_type}s. New number of {data_type}: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(input_json_file) as input_file:\n",
    "        data = json.load(input_file)\n",
    "        issues_data = data['issues']\n",
    "        pull_requests_data = data['pull_requests']\n",
    "        issue_pr_map = data['issue_pr_map']\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing data file found. Fetching data from scratch. This can be very time consuming.\\n\")\n",
    "\n",
    "    print(\"\\nFetching issues ...\")\n",
    "    fetch_issues(issues_data)\n",
    "    print(\"Done fetching issues\")\n",
    "    remove_duplicates(issues_data, \"issue\")\n",
    "\n",
    "    print(\"\\nNow fetching pull requests ...\")\n",
    "    fetch_pull_requests(pull_requests_data)\n",
    "    print(\"Done fetching pull requests\")\n",
    "    remove_duplicates(pull_requests_data, \"pull request\")\n",
    "\n",
    "    # Map issues to pull requests\n",
    "    print(\"\\nMapping issues to pull requests...\")\n",
    "    map_issues_to_prs()\n",
    "    print(\"Done mapping issues to pull requests\")\n",
    "\n",
    "    # Save final data if no exception occurred\n",
    "    # save_data()\n",
    "    save_data(partial=True)\n",
    "\n",
    "    # # Don't run with partial = False until fully tested that everything works corectly\n",
    "    # print(\"Exporting data...\")\n",
    "    # save_data(partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all data on issues and pull requests from Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not input_json_file:\n",
    "#     # lines below fetch all data - Don't run this unless you want to fetch new data (last time it took 740min 59.7s) - better way is to only fetch new data, which should be accomplished by adding the leength of the issues and PR data since the order of those should always be the same (at least it was the last time) - at least thats what I thought ... but when testing it it did fetch some issues that were already fetched ... since my duplicate cleanup lower, reduced the number of issues from 19072 back to 19049 (the original amount on the day I first completed fetching the data ... as of today there are 19072 issues, but the new ones didnt get fetched, so I will have to investigate why that is the case under the assumption that I can't rely on the order for issues and PRs staying the same -> TODO: extract a list with all issue numbers/ PR numbers and then only fetch the ones that are not in the list yet) Also please tell me that new issues and PRs are added to the beginning opposed to the end, since that would mean that my code should never have worked properly\n",
    "\n",
    "#     print(\"\\nFetching issues ...\")\n",
    "#     fetch_issues(issues_data)\n",
    "#     print(\"Done fetching issues\")\n",
    "#     remove_duplicates(issues_data, \"issue\")\n",
    "\n",
    "\n",
    "#     print(\"\\nNow fetching pull requests ...\")\n",
    "#     fetch_pull_requests(pull_requests_data)\n",
    "#     print(\"Done fetching pull requests\")\n",
    "#     remove_duplicates(pull_requests_data, \"pull request\")\n",
    "\n",
    "#     # Map issues to pull requests\n",
    "#     print(\"\\nMapping issues to pull requests...\")\n",
    "#     map_issues_to_prs()\n",
    "#     print(\"Done mapping issues to pull requests\")\n",
    "\n",
    "#     # Save final data if no exception occurred\n",
    "#     # save_data()\n",
    "#     save_data(partial=True)\n",
    "\n",
    "#     # # Don't run with partial = False until fully tested that everything works corectly\n",
    "#     # print(\"Exporting data...\")\n",
    "#     # save_data(partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(issues_data), len(pull_requests_data), len(issue_pr_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CARE: Not well tested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     fetch_issues()\n",
    "# except Exception as e:\n",
    "#     print(f'Exception \"{e}\" occurred while fetching issues. Saving partial data...')\n",
    "#     save_data(partial=True)\n",
    "#     raise\n",
    "\n",
    "# try:\n",
    "#     fetch_pull_requests()\n",
    "# except Exception as e:\n",
    "#     print(f'Exception \"{e}\" occurred while fetching pull requests. Saving partial data...')\n",
    "#     save_data(partial=True)\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# print(f\"Number of issues: {len(data['issues'])}\")\n",
    "print(f\"Number of issues: {len(issues_data)}\")\n",
    "\n",
    "# Extract all issue numbers from the dict and then remove all dupllicates by making it a set and then converting it back to a list\n",
    "# issue_numbers = list(set([issue['number'] for issue in data['issues']]))\n",
    "issue_numbers = list(set([issue['number'] for issue in issues_data]))\n",
    "print(f\"Number of unique issues: {len(issue_numbers)}\")\n",
    "print(f\"Number of duplicate issues: {len(issues_data) - len(issue_numbers)}\")\n",
    "\n",
    "# Track issue numbers and their occurrences\n",
    "issue_tracker = defaultdict(list)\n",
    "\n",
    "for issue in issues_data:\n",
    "    issue_tracker[issue['number']].append(issue)\n",
    "\n",
    "# Identify duplicates\n",
    "duplicates = {number: issues for number, issues in issue_tracker.items() if len(issues) > 1}\n",
    "\n",
    "# Print duplicates\n",
    "print(f\"Duplicates: {list(duplicates.keys())}\")\n",
    "\n",
    "# If there are duplicates, print the detailed information of these issues\n",
    "if duplicates:\n",
    "    print(\"\\nDetails of duplicate issues:\")\n",
    "    for number, issues in duplicates.items():\n",
    "        print(f\"Issue number: {number}\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "\n",
    "# Clean the duplicates under the assumption that just like the last time only the value for reactions differs\n",
    "cleaned_issues_data = []\n",
    "for number, issues in issue_tracker.items():\n",
    "    if len(issues) > 1:\n",
    "        duplicates[number] = issues\n",
    "        # Keep the issue with the highest 'reactions' value\n",
    "        best_issue = max(issues, key=lambda x: x['reactions'])\n",
    "        cleaned_issues_data.append(best_issue)\n",
    "    else:\n",
    "        cleaned_issues_data.append(issues[0])\n",
    "\n",
    "print(f\"\\nNumber of cleaned issues: {len(cleaned_issues_data)}\")\n",
    "\n",
    "# print the cleaned issues data (all entries, whichs number is in the duplicates dict)\n",
    "print(\"\\nCleaned duplicate issues:\")\n",
    "for issue in cleaned_issues_data:\n",
    "    if issue['number'] in duplicates:\n",
    "        print(issue)\n",
    "\n",
    "issue_numbers = [issue['number'] for issue in cleaned_issues_data]\n",
    "print(f\"\\nNumber of issues: {len(cleaned_issues_data)}\")\n",
    "print(f\"Number of unique issues: {len(set(issue_numbers))}\")\n",
    "\n",
    "print(f\"Smallest issue number: {min(issue_numbers)}\")\n",
    "print(f\"Biggest issue number: {max(issue_numbers)}\")\n",
    "difference = max(issue_numbers) - min(issue_numbers)\n",
    "print(f\"Difference between biggest and smallest issue: {difference}\")\n",
    "print(f\"Number of unused issue numbers: {difference - len(issue_numbers) + 1}\")\n",
    "\n",
    "issues_data = cleaned_issues_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map issues to pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map issues to pull requests\n",
    "print(\"Mapping issues to pull requests...\")\n",
    "map_issues_to_prs()\n",
    "print(\"Done mapping issues to pull requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save final data if no exception occurred\n",
    "# save_data()\n",
    "\n",
    "# Don't run with partial = False until fully tested that everything works corectly\n",
    "# print(\"Exporting data...\")\n",
    "# save_data(partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull request test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch a single pull request for testing\n",
    "# pr_number = 19489\n",
    "# try:\n",
    "#     pr = repo.get_pull(pr_number)\n",
    "#     issue = repo.get_issue(pr_number)  # Fetch the corresponding issue for reactions\n",
    "#     pr_files = pr.get_files()\n",
    "#     files_changed = [{\"filename\": f.filename, \"additions\": f.additions, \"deletions\": f.deletions} for f in pr_files]\n",
    "#     pr_commits = pr.get_commits()\n",
    "#     commit_data = [{\n",
    "#         \"sha\": commit.sha,\n",
    "#         \"author\": commit.commit.author.name,\n",
    "#         \"date\": commit.commit.author.date.isoformat(),\n",
    "#         \"message\": commit.commit.message\n",
    "#     } for commit in pr_commits]\n",
    "#     pull_requests_data = [{\n",
    "#         \"number\": pr.number,\n",
    "#         \"title\": pr.title,\n",
    "#         \"state\": pr.state,\n",
    "#         \"created_at\": pr.created_at.isoformat(),\n",
    "#         \"merged_at\": pr.merged_at.isoformat() if pr.merged_at else None,\n",
    "#         \"closed_at\": pr.closed_at.isoformat() if pr.closed_at else None,\n",
    "#         \"user\": pr.user.login,\n",
    "#         \"comments\": pr.comments,\n",
    "#         \"review_comments\": pr.review_comments,\n",
    "#         \"description\": pr.body,\n",
    "#         \"description_length\": len(pr.body) if pr.body else 0,\n",
    "#         \"additions\": pr.additions,\n",
    "#         \"deletions\": pr.deletions,\n",
    "#         \"changed_files\": pr.changed_files,\n",
    "#         \"files\": files_changed,\n",
    "#         \"commits\": commit_data,\n",
    "#         \"labels\": [label.name for label in pr.labels],\n",
    "#         \"mergeable_state\": pr.mergeable_state\n",
    "#     }]\n",
    "#     print(\"Pull request data collected successfully.\")\n",
    "# except UnknownObjectException:\n",
    "#     print(f\"Pull request #{pr_number} not found.\")\n",
    "# except RateLimitExceededException:\n",
    "#     print(\"Rate limit exceeded. Please wait and try again later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_difference(created_at, closed_at):\n",
    "    # Define the format of the timestamps\n",
    "    timestamp_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "    \n",
    "    # Parse the created_at timestamp\n",
    "    created_time = datetime.strptime(created_at, timestamp_format)\n",
    "    \n",
    "    # Parse the closed_at timestamp if it is not None\n",
    "    if closed_at is not None:\n",
    "        closed_time = datetime.strptime(closed_at, timestamp_format)\n",
    "        # Calculate the difference\n",
    "        time_difference = (closed_time - created_time).total_seconds() # Calculate the time difference and convert it to seconds\n",
    "    else:\n",
    "        time_difference = None  # Or some other placeholder to indicate it's still open\n",
    "    \n",
    "    return time_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_turnaround_time(data):\n",
    "\n",
    "    for entry in data:\n",
    "        entry['turnaround_time'] = None # Initialize the turnaround time to handle open issues\n",
    "        if entry['closed_at']: # Only calculate the turnaround time if the entry is closed\n",
    "            entry['turnaround_time'] = int(calculate_time_difference(entry['created_at'], entry['closed_at']))\n",
    "        # print(entry['turnaround_time'])\n",
    "\n",
    "    # Verify all 'turnaround time' values are integers or None\n",
    "    for entry in data:\n",
    "        if entry['turnaround_time'] is not None:\n",
    "            assert isinstance(entry['turnaround_time'], int), f\"Turnaround time is not an int: {entry['turnaround_time']}\"\n",
    "\n",
    "calculate_turnaround_time(issues_data)\n",
    "calculate_turnaround_time(pull_requests_data)\n",
    "\n",
    "# save_data(partial=True) # Don't want to overwrite the original data file until I am done with data processing and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(partial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly example below\n",
    "\n",
    "It isn't working as intended yet, but still can give an idea on how it can be done.\n",
    "It is currently uncommented!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# plot_configs = {\n",
    "#     \"issue_turnaround_time\": {\n",
    "#         \"function\": \"plot_turnaround_time\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x_axis\": \"Creation Date (resampled monthly)\",\n",
    "#         \"y_axis\": \"Average Turnaround Time (days)\",\n",
    "#         \"description\": \"Average turnaround time over time for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_turnaround_time\": {\n",
    "#         \"function\": \"plot_turnaround_time\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x_axis\": \"Creation Date (resampled monthly)\",\n",
    "#         \"y_axis\": \"Average Turnaround Time (days)\",\n",
    "#         \"description\": \"Average turnaround time over time for Pull Requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_turnaround_time_scatter\": {\n",
    "#         \"function\": \"plot_turnaround_time_scatter\",\n",
    "#         \"plot_type\": \"scatter\",\n",
    "#         \"x_axis\": \"Creation Date\",\n",
    "#         \"y_axis\": \"Turnaround Time (days)\",\n",
    "#         \"description\": \"Scatter plot of turnaround time over time for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_turnaround_time_scatter\": {\n",
    "#         \"function\": \"plot_turnaround_time_scatter\",\n",
    "#         \"plot_type\": \"scatter\",\n",
    "#         \"x_axis\": \"Creation Date\",\n",
    "#         \"y_axis\": \"Turnaround Time (days)\",\n",
    "#         \"description\": \"Scatter plot of turnaround time over time for Pull Requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_open_to_closed_ratio\": {\n",
    "#         \"function\": \"plot_open_to_closed_ratio\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Open to Closed Ratio\",\n",
    "#         \"description\": \"Ratio of cumulative open to closed issues for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_open_to_closed_ratio\": {\n",
    "#         \"function\": \"plot_open_to_closed_ratio\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Open to Closed Ratio\",\n",
    "#         \"description\": \"Ratio of cumulative open to closed issues for Pull Requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_open_and_closed\": {\n",
    "#         \"function\": \"plot_open_and_closed\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Cumulative count of Issues\",\n",
    "#         \"description\": \"Cumulative open and closed issues for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"pull request_open_and_closed\": {\n",
    "#         \"function\": \"plot_open_and_closed\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Cumulative count of Pull Requests\",\n",
    "#         \"description\": \"Cumulative open and closed pull requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"issue_open_counts\": {\n",
    "#         \"function\": \"plot_open_counts\",\n",
    "#         \"plot_type\": \"bar\",\n",
    "#         \"x_axis\": \"Creation Date (grouped by month)\",\n",
    "#         \"y_axis\": \"Count of Open Issues\",\n",
    "#         \"description\": \"Monthly count of open issues for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_open_counts\": {\n",
    "#         \"function\": \"plot_open_counts\",\n",
    "#         \"plot_type\": \"bar\",\n",
    "#         \"x_axis\": \"Creation Date (grouped by month)\",\n",
    "#         \"y_axis\": \"Count of Open Pull Requests\",\n",
    "#         \"description\": \"Monthly count of open pull requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"commit_frequency_and_size\": {\n",
    "#         \"function\": \"plot_commit_frequency_and_size\",\n",
    "#         \"plot_type\": \"bar + line dual axis\",\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": {\n",
    "#             \"left\": \"Monthly Commits\",\n",
    "#             \"right\": \"Average Commit Size (lines)\"\n",
    "#         },\n",
    "#         \"description\": \"Commit frequency and average commit size over time (unfiltered)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"commit_frequency_and_size_filtered\": {\n",
    "#         \"function\": \"plot_commit_frequency_and_size\",\n",
    "#         \"plot_type\": \"bar + line dual axis\",\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": {\n",
    "#             \"left\": \"Monthly Commits\",\n",
    "#             \"right\": \"Average Commit Size (lines)\"\n",
    "#         },\n",
    "#         \"description\": \"Commit frequency and average commit size over time (filtered)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"requests_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Requests'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"impact_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Impact'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"severity_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Severity'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"code_quality_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Code Quality'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"commit_metrics\": {\n",
    "#         \"function\": \"plot_commit_metrics_with_bugs\",\n",
    "#         \"plot_type\": \"bar + line dual axis with third axis\",\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": {\n",
    "#             \"left\": \"Monthly Commits\",\n",
    "#             \"right\": \"Average Commit Size (lines)\"\n",
    "#         },\n",
    "#         \"description\": \"Commit metrics (frequency and size) over time for pull requests (commit data)\",\n",
    "#         \"export_formats\": [\"csv/json\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"bug_metrics\": {\n",
    "#         \"function\": \"plot_commit_metrics_with_bugs\",\n",
    "#         \"plot_type\": \"bar + line dual axis with third axis\",\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": \"Bug Issues Count\",\n",
    "#         \"description\": \"Monthly bug issues count (from issues with 'bug' label)\",\n",
    "#         \"export_formats\": [\"csv/json\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open('plot_configs.json', 'w') as f:\n",
    "#     json.dump(plot_configs, f, indent=4)\n",
    "\n",
    "# print(\"Plot configuration saved as plot_configs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# plot_configs = {\n",
    "#     \"issue_turnaround_time\": {\n",
    "#         \"function\": \"plot_turnaround_time\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x\": \"created_at\",           # column with creation date\n",
    "#         \"y\": \"turnaround_time\",      # column with turnaround time\n",
    "#         \"x_axis\": \"Creation Date\",\n",
    "#         \"y_axis\": \"Average Turnaround Time (days)\",\n",
    "#         \"description\": \"Average turnaround time over time for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_turnaround_time\": {\n",
    "#         \"function\": \"plot_turnaround_time\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"turnaround_time\",\n",
    "#         \"x_axis\": \"Creation Date\",\n",
    "#         \"y_axis\": \"Average Turnaround Time (days)\",\n",
    "#         \"description\": \"Average turnaround time over time for Pull Requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_turnaround_time_scatter\": {\n",
    "#         \"function\": \"plot_turnaround_time_scatter\",\n",
    "#         \"plot_type\": \"scatter\",\n",
    "#         # To swap the axes, we now explicitly specify which column goes where:\n",
    "#         # \"x\": \"turnaround_time\",\n",
    "#         # \"y\": \"created_at\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"turnaround_time\",\n",
    "#         \"x_axis\": \"Turnaround Time (days)\",\n",
    "#         \"y_axis\": \"Creation Date\",\n",
    "#         \"description\": \"Scatter plot of turnaround time vs. creation date for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_turnaround_time_scatter\": {\n",
    "#         \"function\": \"plot_turnaround_time_scatter\",\n",
    "#         \"plot_type\": \"scatter\",\n",
    "#         # \"x\": \"turnaround_time\",\n",
    "#         # \"y\": \"created_at\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"turnaround_time\",\n",
    "#         \"x_axis\": \"Turnaround Time (days)\",\n",
    "#         \"y_axis\": \"Creation Date\",\n",
    "#         \"description\": \"Scatter plot of turnaround time vs. creation date for Pull Requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_open_to_closed_ratio\": {\n",
    "#         \"function\": \"plot_open_to_closed_ratio\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x\": \"time\",                   # assuming the resampled data has a 'time' column or use index\n",
    "#         \"y\": \"open_to_closed_ratio\",   # ratio values\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Open to Closed Ratio\",\n",
    "#         \"description\": \"Cumulative ratio of open to closed issues for Issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_open_to_closed_ratio\": {\n",
    "#         \"function\": \"plot_open_to_closed_ratio\",\n",
    "#         \"plot_type\": \"line\",\n",
    "#         \"x\": \"time\",\n",
    "#         \"y\": \"open_to_closed_ratio\",\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": \"Open to Closed Ratio\",\n",
    "#         \"description\": \"Cumulative ratio of open to closed pull requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"issue_open_and_closed\": {\n",
    "#         \"function\": \"plot_open_and_closed\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"time\",  \n",
    "#         \"y\": { \"left\": \"cumulative_open\", \"right\": \"cumulative_closed\" },\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": { \"left\": \"Cumulative Open\", \"right\": \"Cumulative Closed\" },\n",
    "#         \"description\": \"Cumulative open and closed issues for Issues (linear scale)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"pull request_open_and_closed\": {\n",
    "#         \"function\": \"plot_open_and_closed\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"time\",\n",
    "#         \"y\": { \"left\": \"cumulative_open\", \"right\": \"cumulative_closed\" },\n",
    "#         \"x_axis\": \"Time (resampled monthly)\",\n",
    "#         \"y_axis\": { \"left\": \"Cumulative Open\", \"right\": \"Cumulative Closed\" },\n",
    "#         \"description\": \"Cumulative open and closed pull requests (linear scale)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"issue_open_counts\": {\n",
    "#         \"function\": \"plot_open_counts\",\n",
    "#         \"plot_type\": \"bar\",\n",
    "#         \"x\": \"created_at\",  # after grouping, this is the month column\n",
    "#         \"y\": \"count\",       # the count of open issues per month\n",
    "#         \"x_axis\": \"Creation Date (grouped by month)\",\n",
    "#         \"y_axis\": \"Count of Open Issues\",\n",
    "#         \"description\": \"Monthly count of open issues\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"pull request_open_counts\": {\n",
    "#         \"function\": \"plot_open_counts\",\n",
    "#         \"plot_type\": \"bar\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"count\",\n",
    "#         \"x_axis\": \"Creation Date (grouped by month)\",\n",
    "#         \"y_axis\": \"Count of Open Pull Requests\",\n",
    "#         \"description\": \"Monthly count of open pull requests\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"commit_frequency_and_size\": {\n",
    "#         \"function\": \"plot_commit_frequency_and_size\",\n",
    "#         \"plot_type\": \"bar + line dual axis\",\n",
    "#         \"x\": \"date\",  # column with the resampled date\n",
    "#         \"y\": { \"left\": \"monthly_commits\", \"right\": \"avg_size\" },\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": { \"left\": \"Monthly Commits\", \"right\": \"Average Commit Size (lines)\" },\n",
    "#         \"description\": \"Commit frequency and average commit size over time (unfiltered)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"commit_frequency_and_size_filtered\": {\n",
    "#         \"function\": \"plot_commit_frequency_and_size\",\n",
    "#         \"plot_type\": \"bar + line dual axis\",\n",
    "#         \"x\": \"date\",\n",
    "#         \"y\": { \"left\": \"monthly_commits\", \"right\": \"avg_size\" },\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": { \"left\": \"Monthly Commits\", \"right\": \"Average Commit Size (lines)\" },\n",
    "#         \"description\": \"Commit frequency and average commit size over time (filtered)\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\", \"log\"]\n",
    "#     },\n",
    "#     \"requests_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"created_at\",  # column with the date\n",
    "#         \"y\": \"count\",       # count per label after grouping\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Requests'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"impact_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"count\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Impact'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"severity_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"count\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Severity'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"code_quality_label_data\": {\n",
    "#         \"function\": \"plot_label_group\",\n",
    "#         \"plot_type\": \"line (multiple lines)\",\n",
    "#         \"x\": \"created_at\",\n",
    "#         \"y\": \"count\",\n",
    "#         \"x_axis\": \"Time (grouped by month)\",\n",
    "#         \"y_axis\": \"Count per Label\",\n",
    "#         \"description\": \"Monthly counts for label group 'Code Quality'\",\n",
    "#         \"export_formats\": [\"csv/json\", \"png\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"commit_metrics\": {\n",
    "#         \"function\": \"plot_commit_metrics_with_bugs\",\n",
    "#         \"plot_type\": \"bar + line dual axis with third axis\",\n",
    "#         \"x\": \"date\",\n",
    "#         \"y\": { \"left\": \"monthly_commits\", \"right\": \"avg_size\" },\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": { \"left\": \"Monthly Commits\", \"right\": \"Average Commit Size (lines)\" },\n",
    "#         \"description\": \"Commit metrics (frequency and size) over time for pull requests (commit data)\",\n",
    "#         \"export_formats\": [\"csv/json\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     },\n",
    "#     \"bug_metrics\": {\n",
    "#         \"function\": \"plot_commit_metrics_with_bugs\",\n",
    "#         \"plot_type\": \"bar + line dual axis with third axis\",\n",
    "#         \"x\": \"date\",\n",
    "#         \"y\": \"bug_issues\",  # single series for bug issues\n",
    "#         \"x_axis\": \"Date (resampled monthly)\",\n",
    "#         \"y_axis\": \"Bug Issues Count\",\n",
    "#         \"description\": \"Monthly bug issues count (from issues with 'bug' label)\",\n",
    "#         \"export_formats\": [\"csv/json\"],\n",
    "#         \"available_scales\": [\"linear\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open('plot_configs.json', 'w') as f:\n",
    "#     json.dump(plot_configs, f, indent=4)\n",
    "\n",
    "# print(\"Plot configuration saved as plot_configs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # Load the adapted configuration dictionary from JSON file\n",
    "# with open('plot_configs.json', 'r') as f:\n",
    "#     plot_configs = json.load(f)\n",
    "\n",
    "# # Helper: load exported data from CSV; fallback to JSON if CSV not found.\n",
    "# def load_exported_data(key):\n",
    "#     try:\n",
    "#         df = pd.read_csv(f\"{key}.csv\", index_col=0)\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         try:\n",
    "#             df = pd.read_json(f\"{key}.json\")\n",
    "#             return df\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not load data for {key}: {e}\")\n",
    "#             return None\n",
    "\n",
    "# # Iterate over each plot configuration and generate the visualization for each scale option.\n",
    "# for key, config in plot_configs.items():\n",
    "#     df = load_exported_data(key)\n",
    "#     if df is None:\n",
    "#         continue  # Skip if data not found.\n",
    "    \n",
    "#     # Determine which scales to produce; default to \"linear\" if not specified.\n",
    "#     scale_options = config.get(\"available_scales\", [\"linear\"])\n",
    "    \n",
    "#     for scale in scale_options:\n",
    "#         print(f\"Visualizing {key} with {scale} scale: {config['description']}\")\n",
    "        \n",
    "#         # Determine the x-axis data.\n",
    "#         if config.get(\"x\") and config[\"x\"] in df.columns:\n",
    "#             x_data = df[config[\"x\"]]\n",
    "#         else:\n",
    "#             x_data = df.index  # Fallback to index if \"x\" key not found in columns.\n",
    "        \n",
    "#         # If config[\"y\"] is not a dict, we use a simple single-axis plot.\n",
    "#         if not isinstance(config.get(\"y\"), dict):\n",
    "#             if config.get(\"y\") and config[\"y\"] in df.columns:\n",
    "#                 y_data = df[config[\"y\"]]\n",
    "#             else:\n",
    "#                 y_data = df[df.columns[0]]  # Fallback: first column.\n",
    "            \n",
    "#             if config[\"plot_type\"].lower() in [\"line\", \"scatter\"]:\n",
    "#                 if config[\"plot_type\"].lower() == \"line\":\n",
    "#                     fig = px.line(df, x=x_data, y=y_data,\n",
    "#                                   title=f\"{config['description']} ({scale} scale)\",\n",
    "#                                   labels={\"x\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                           \"y\": config.get(\"y_axis\", \"Y Axis\")})\n",
    "#                 else:  # scatter\n",
    "#                     fig = px.scatter(df, x=x_data, y=y_data,\n",
    "#                                      title=f\"{config['description']} ({scale} scale)\",\n",
    "#                                      labels={\"x\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                              \"y\": config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             elif config[\"plot_type\"].lower() == \"bar\":\n",
    "#                 fig = px.bar(df, x=x_data, y=y_data,\n",
    "#                              title=f\"{config['description']} ({scale} scale)\",\n",
    "#                              labels={\"x\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                      \"y\": config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             elif \"multiple lines\" in config[\"plot_type\"].lower():\n",
    "#                 # Plot all columns as separate lines.\n",
    "#                 fig = px.line(df, x=x_data, y=df.columns,\n",
    "#                               title=f\"{config['description']} ({scale} scale)\",\n",
    "#                               labels={\"x\": config.get(\"x_axis\", \"X Axis\"), \"y\": \"Values\"})\n",
    "#             else:\n",
    "#                 # Default fallback to a line plot.\n",
    "#                 fig = px.line(df, x=x_data, y=y_data,\n",
    "#                               title=f\"{config['description']} ({scale} scale)\",\n",
    "#                               labels={\"x\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                       \"y\": config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             if scale == \"log\":\n",
    "#                 fig.update_yaxes(type=\"log\")\n",
    "        \n",
    "#         # Dual-axis case: config[\"y\"] is a dict.\n",
    "#         else:\n",
    "#             # Create a subplot with secondary y-axis.\n",
    "#             fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "#             y_left_key = config[\"y\"].get(\"left\")\n",
    "#             y_right_key = config[\"y\"].get(\"right\")\n",
    "#             if y_left_key not in df.columns or y_right_key not in df.columns:\n",
    "#                 print(f\"Missing columns for dual-axis plot in {key}\")\n",
    "#                 continue\n",
    "#             # Left trace: use a bar plot.\n",
    "#             fig.add_trace(\n",
    "#                 go.Bar(x=x_data, y=df[y_left_key],\n",
    "#                        name=config[\"y_axis\"].get(\"left\", \"Left Y\")),\n",
    "#                 secondary_y=False\n",
    "#             )\n",
    "#             # Right trace: use a line plot.\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(x=x_data, y=df[y_right_key],\n",
    "#                            name=config[\"y_axis\"].get(\"right\", \"Right Y\")),\n",
    "#                 secondary_y=True\n",
    "#             )\n",
    "#             fig.update_layout(title_text=f\"{config['description']} ({scale} scale)\")\n",
    "#             fig.update_xaxes(title_text=config.get(\"x_axis\", \"X Axis\"))\n",
    "#             fig.update_yaxes(title_text=config[\"y_axis\"].get(\"left\", \"Left Y\"), secondary_y=False)\n",
    "#             fig.update_yaxes(title_text=config[\"y_axis\"].get(\"right\", \"Right Y\"), secondary_y=True, type=scale)\n",
    "        \n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # Load the adapted configuration dictionary from JSON file\n",
    "# with open('plot_configs.json', 'r') as f:\n",
    "#     plot_configs = json.load(f)\n",
    "\n",
    "# # Helper: load exported data from CSV; fallback to JSON if CSV not found.\n",
    "# def load_exported_data(key):\n",
    "#     try:\n",
    "#         df = pd.read_csv(f\"{key}.csv\", index_col=0)\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         try:\n",
    "#             df = pd.read_json(f\"{key}.json\")\n",
    "#             return df\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not load data for {key}: {e}\")\n",
    "#             return None\n",
    "\n",
    "# # Iterate over each plot configuration and generate the visualization for each scale option.\n",
    "# for key, config in plot_configs.items():\n",
    "#     df = load_exported_data(key)\n",
    "#     if df is None:\n",
    "#         continue  # Skip if data not found.\n",
    "    \n",
    "#     # Determine which scales to produce; default to \"linear\" if not specified.\n",
    "#     scale_options = config.get(\"available_scales\", [\"linear\"])\n",
    "    \n",
    "#     for scale in scale_options:\n",
    "#         print(f\"Visualizing {key} with {scale} scale: {config['description']}\")\n",
    "        \n",
    "#         # Determine x-axis: use the column name from config if available; else fallback to index.\n",
    "#         x_col = config.get(\"x\")\n",
    "#         if x_col and x_col in df.columns:\n",
    "#             x_data = x_col  # use column name for Plotly Express\n",
    "#         else:\n",
    "#             x_data = None  # fallback to index\n",
    "        \n",
    "#         # Single-axis case: config[\"y\"] is not a dict.\n",
    "#         if not isinstance(config.get(\"y\"), dict):\n",
    "#             y_col = config.get(\"y\")\n",
    "#             if y_col and y_col in df.columns:\n",
    "#                 y_data = y_col  # use column name\n",
    "#             else:\n",
    "#                 y_data = df.columns[0]  # fallback: first column\n",
    "            \n",
    "#             # For Plotly Express, if x_data is None, it will use the DataFrame index.\n",
    "#             if config[\"plot_type\"].lower() in [\"line\", \"scatter\"]:\n",
    "#                 if config[\"plot_type\"].lower() == \"line\":\n",
    "#                     fig = px.line(df, x=x_data, y=y_data,\n",
    "#                                   title=f\"{config['description']} ({scale} scale)\",\n",
    "#                                   labels={x_data if x_data else \"index\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                           y_data: config.get(\"y_axis\", \"Y Axis\")})\n",
    "#                 else:  # scatter\n",
    "#                     fig = px.scatter(df, x=x_data, y=y_data,\n",
    "#                                      title=f\"{config['description']} ({scale} scale)\",\n",
    "#                                      labels={x_data if x_data else \"index\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                              y_data: config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             elif config[\"plot_type\"].lower() == \"bar\":\n",
    "#                 fig = px.bar(df, x=x_data, y=y_data,\n",
    "#                              title=f\"{config['description']} ({scale} scale)\",\n",
    "#                              labels={x_data if x_data else \"index\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                      y_data: config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             elif \"multiple lines\" in config[\"plot_type\"].lower():\n",
    "#                 # Plot all columns as separate lines.\n",
    "#                 fig = px.line(df, x=x_data, y=df.columns,\n",
    "#                               title=f\"{config['description']} ({scale} scale)\",\n",
    "#                               labels={x_data if x_data else \"index\": config.get(\"x_axis\", \"X Axis\"), \n",
    "#                                       \"value\": \"Values\"})\n",
    "#             else:\n",
    "#                 # Default fallback to a line plot.\n",
    "#                 fig = px.line(df, x=x_data, y=y_data,\n",
    "#                               title=f\"{config['description']} ({scale} scale)\",\n",
    "#                               labels={x_data if x_data else \"index\": config.get(\"x_axis\", \"X Axis\"),\n",
    "#                                       y_data: config.get(\"y_axis\", \"Y Axis\")})\n",
    "#             if scale == \"log\":\n",
    "#                 fig.update_yaxes(type=\"log\")\n",
    "        \n",
    "#         # Dual-axis case: config[\"y\"] is a dict.\n",
    "#         else:\n",
    "#             fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "#             y_left_key = config[\"y\"].get(\"left\")\n",
    "#             y_right_key = config[\"y\"].get(\"right\")\n",
    "#             if y_left_key not in df.columns or y_right_key not in df.columns:\n",
    "#                 print(f\"Missing columns for dual-axis plot in {key}\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Use x_data if available; otherwise, use the index.\n",
    "#             if x_col and x_col in df.columns:\n",
    "#                 x_vals = df[x_col]\n",
    "#             else:\n",
    "#                 x_vals = df.index\n",
    "            \n",
    "#             # Left trace: bar plot.\n",
    "#             fig.add_trace(\n",
    "#                 go.Bar(x=x_vals, y=df[y_left_key],\n",
    "#                        name=config[\"y_axis\"].get(\"left\", \"Left Y\")),\n",
    "#                 secondary_y=False\n",
    "#             )\n",
    "#             # Right trace: line plot.\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(x=x_vals, y=df[y_right_key],\n",
    "#                            name=config[\"y_axis\"].get(\"right\", \"Right Y\")),\n",
    "#                 secondary_y=True\n",
    "#             )\n",
    "#             fig.update_layout(title_text=f\"{config['description']} ({scale} scale)\")\n",
    "#             fig.update_xaxes(title_text=config.get(\"x_axis\", \"X Axis\"))\n",
    "#             fig.update_yaxes(title_text=config[\"y_axis\"].get(\"left\", \"Left Y\"), secondary_y=False)\n",
    "#             fig.update_yaxes(title_text=config[\"y_axis\"].get(\"right\", \"Right Y\"), secondary_y=True, type=scale)\n",
    "        \n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly example above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script continues below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(df, filename, export_format=EXPORT_FORMAT):\n",
    "    if export_format == 'both':\n",
    "        df.to_json(f'{filename}.json', orient='records', date_format='iso')\n",
    "        df.to_csv(f'{filename}.csv')\n",
    "        print(f\"Data exported as {export_format} to '{filename}.json' and '{filename}.csv'.\")\n",
    "    elif export_format == 'json':\n",
    "        df.to_json(export_filename, orient='records', date_format='iso')\n",
    "        print(f\"Data exported as {export_format} to '{filename}.json'.\")\n",
    "    elif export_format == 'csv':\n",
    "        df.to_csv(export_filename)\n",
    "        print(f\"Data exported as {export_format} to '{filename}.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_turnaround_time(data):\n",
    "    plot_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if entry['turnaround_time'] is not None:\n",
    "            created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            if entry['turnaround_time']: # TODO: Do I really want to ignore entries, that are still open? Perhaps it might be better to just set the turnaround time to the current date - created_at OR do I just wanna plot them seperately to indicate that there are (long left) open issues/ PRs\n",
    "                turnaround_time_days = entry['turnaround_time'] / (60 * 60 * 24)  # Convert seconds to days\n",
    "                plot_data.append((created_at, turnaround_time_days))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(plot_data, columns=['created_at', 'turnaround_time'])\n",
    "\n",
    "    # Convert the 'created_at' column to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_turnaround_time_yearly(data, data_type):\n",
    "    \n",
    "    df = calculate_turnaround_time(data)\n",
    "\n",
    "    # Set the index to the creation date\n",
    "    df.set_index('created_at', inplace=True)\n",
    "\n",
    "    # Resample the data by month and calculate the average turnaround time\n",
    "    df_resampled = df.resample('M').mean()\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_resampled.index, df_resampled['turnaround_time'], marker='o', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Average {data_type} Turnaround Time Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel('Average Turnaround Time (days)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Optional: improving the date formatting\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "print(\"Issues: \")\n",
    "plot_turnaround_time_yearly(issues_data, \"Issue\")\n",
    "print(\"Pull Requests:\")\n",
    "plot_turnaround_time_yearly(pull_requests_data, \"Pull Request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_turnaround_time(data, data_type):\n",
    "    \n",
    "    df = calculate_turnaround_time(data)\n",
    "\n",
    "    # Set the index to the creation date\n",
    "    df.set_index('created_at', inplace=True)\n",
    "\n",
    "    # Resample the data by month and calculate the average turnaround time\n",
    "    df_resampled = df.resample('M').mean()\n",
    "\n",
    "    # Define the number of months per x-axis label\n",
    "    months_per_label = 3  # change this to any number of months that should be one label\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(df_resampled.index, df_resampled['turnaround_time'], marker='o', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Average {data_type} Turnaround Time Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel('Average Turnaround Time (days)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))  # Set major ticks to every custom number of months\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "print(\"Issues:\")\n",
    "plot_turnaround_time(issues_data, \"Issue\")\n",
    "print(\"Pull Requests:\")\n",
    "plot_turnaround_time(pull_requests_data, \"Pull Request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_turnaround_time(data, data_type, export_format=EXPORT_FORMAT):\n",
    "    # Calculate the turnaround time\n",
    "    df = calculate_turnaround_time(data)\n",
    "    # Set the index to the creation date\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    # Resample the data by month and calculate the average turnaround time\n",
    "    df_resampled = df.resample('M').mean()\n",
    "\n",
    "    export_df(df_resampled, f'{data_type.lower()}_turnaround_time', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(df_resampled.index, df_resampled['turnaround_time'], marker='o', linestyle='-')\n",
    "    \n",
    "    # Formatting the plot\n",
    "    plt.title(f'Average {data_type} Turnaround Time Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel('Average Turnaround Time (days)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    months_per_label = 3  # Adjust as desired\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{data_type.lower()}_turnaround_time.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "print(\"Issues:\")\n",
    "plot_turnaround_time(issues_data, \"Issue\", export_format=EXPORT_FORMAT)\n",
    "print(\"Pull Requests:\")\n",
    "plot_turnaround_time(pull_requests_data, \"Pull Request\", export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_turnaround_time_scatter(data, data_type):\n",
    "    \n",
    "    df = calculate_turnaround_time(data)\n",
    "\n",
    "    # Set the index to the creation date (optional, not necessary for scatter plot)\n",
    "    # df.set_index('created_at', inplace=False)\n",
    "\n",
    "    # Define the number of months per x-axis label\n",
    "    months_per_label = 3  # You can change this to any number of months you want\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.scatter(df['created_at'], df['turnaround_time'], alpha=0.5)\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'{data_type} Turnaround Time Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel('Turnaround Time (days)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))  # Set major ticks to every custom number of months\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "print(\"Issues: \")\n",
    "plot_turnaround_time_scatter(issues_data, \"Issue\")\n",
    "print(\"Pull Requests:\")\n",
    "plot_turnaround_time_scatter(pull_requests_data, \"Pull Request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_turnaround_time_scatter(data, data_type, export_format=EXPORT_FORMAT):\n",
    "    # Calculate the turnaround time\n",
    "    df = calculate_turnaround_time(data)\n",
    "\n",
    "    # Export the data if specified\n",
    "    export_df(df, f'{data_type.lower()}_turnaround_time_scatter', export_format)\n",
    "\n",
    "    # Plotting the scatter plot (for visualization, if needed)\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.scatter(df['created_at'], df['turnaround_time'], alpha=0.5)\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'{data_type} Turnaround Time Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel('Turnaround Time (days)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    months_per_label = 3  # Adjust as desired\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{data_type.lower()}_turnaround_time_scatter.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "print(\"Issues:\")\n",
    "plot_turnaround_time_scatter(issues_data, \"Issue\", export_format=EXPORT_FORMAT)\n",
    "print(\"Pull Requests:\")\n",
    "plot_turnaround_time_scatter(pull_requests_data, \"Pull Request\", export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Careful!\n",
    "\n",
    "The plots above seem to indicate that the \"health\" of the repository got better over time. BUT for turnaround time open issues are completely ignored, which skews the representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find out the number of issues that have been opened/ closed\n",
    "# open_issues = 0\n",
    "# closed_issues = 0\n",
    "# for issue in issues_data:\n",
    "#     if issue['state'] == 'open':\n",
    "#         open_issues += 1\n",
    "#     elif issue['state'] == 'closed':\n",
    "#         closed_issues += 1\n",
    "#     else:\n",
    "#         print(f\"Issue #{issue['number']} has an unknown state: {issue['state']}\")\n",
    "# print(f\"Number of open issues: {open_issues}\")\n",
    "# print(f\"Number of closed issues: {closed_issues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_open_to_closed_ratio(data):\n",
    "    # Extract relevant data\n",
    "    plot_data = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        closed_at = entry['closed_at']\n",
    "        if closed_at:\n",
    "            closed_at = datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        plot_data.append((created_at, closed_at, state))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(plot_data, columns=['created_at', 'closed_at', 'state'])\n",
    "\n",
    "    # Convert dates to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['closed_at'] = pd.to_datetime(df['closed_at'])\n",
    "\n",
    "    # Resample data by month\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.resample('M').apply({\n",
    "        'state': lambda x: x.value_counts().get('open', 0),\n",
    "        'closed_at': lambda x: x.notnull().sum()\n",
    "    }).rename(columns={'state': 'open_issues', 'closed_at': 'closed_issues'})\n",
    "\n",
    "    # Calculate cumulative sums\n",
    "    monthly_data['cumulative_open'] = monthly_data['open_issues'].cumsum()\n",
    "    monthly_data['cumulative_closed'] = monthly_data['closed_issues'].cumsum()\n",
    "\n",
    "    # Calculate the ratio\n",
    "    monthly_data['open_to_closed_ratio'] = monthly_data['cumulative_open'] / monthly_data['cumulative_closed']\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "def plot_open_to_closed_ratio(data, data_type):\n",
    "\n",
    "    monthly_data = calculate_open_to_closed_ratio(data)\n",
    "\n",
    "    # Define the number of months per x-axis label\n",
    "    months_per_label = 3  # change this to any number of months that should be one label\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(monthly_data.index, monthly_data['open_to_closed_ratio'], marker='o', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Open vs. Closed {data_type} Ratio Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'Open to Closed {data_type} Ratio')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))  # Set major ticks to every custom number of months\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_open_to_closed_ratio(issues_data, 'Issue')\n",
    "plot_open_to_closed_ratio(pull_requests_data, 'Pull Request')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_open_to_closed_ratio(data):\n",
    "    # Extract relevant data\n",
    "    plot_data = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        closed_at = entry['closed_at']\n",
    "        if closed_at:\n",
    "            closed_at = datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        plot_data.append((created_at, closed_at, state))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(plot_data, columns=['created_at', 'closed_at', 'state'])\n",
    "\n",
    "    # Resample data by month and calculate metrics\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.resample('M').apply({\n",
    "        'state': lambda x: x.value_counts().get('open', 0),\n",
    "        'closed_at': lambda x: x.notnull().sum()\n",
    "    }).rename(columns={'state': 'open_issues', 'closed_at': 'closed_issues'})\n",
    "\n",
    "    # Calculate cumulative sums and the ratio\n",
    "    monthly_data['cumulative_open'] = monthly_data['open_issues'].cumsum()\n",
    "    monthly_data['cumulative_closed'] = monthly_data['closed_issues'].cumsum()\n",
    "    monthly_data['open_to_closed_ratio'] = monthly_data['cumulative_open'] / monthly_data['cumulative_closed']\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "def plot_open_to_closed_ratio(data, data_type, export_format=EXPORT_FORMAT):\n",
    "    # Generate data for plotting\n",
    "    monthly_data = calculate_open_to_closed_ratio(data)\n",
    "\n",
    "    # Export the data if specified\n",
    "    export_df(monthly_data, f'{data_type.lower()}_open_to_closed_ratio', export_format)\n",
    "\n",
    "    # Plot the ratio (for visualization, if needed)\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(monthly_data.index, monthly_data['open_to_closed_ratio'], marker='o', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Open vs. Closed {data_type} Ratio Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'Open to Closed {data_type} Ratio')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    months_per_label = 3  # Adjust as desired\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{data_type.lower()}_open_to_closed_ratio.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_open_to_closed_ratio(issues_data, 'Issue', export_format=EXPORT_FORMAT)\n",
    "plot_open_to_closed_ratio(pull_requests_data, 'Pull Request', export_format=EXPORT_FORMAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_open_and_closed(data):\n",
    "    plot_data = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        closed_at = entry['closed_at']\n",
    "        if closed_at:\n",
    "            closed_at = datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        plot_data.append((created_at, closed_at, state))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(plot_data, columns=['created_at', 'closed_at', 'state'])\n",
    "\n",
    "    # Convert dates to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['closed_at'] = pd.to_datetime(df['closed_at'])\n",
    "\n",
    "    # Resample data by month\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.resample('M').apply({\n",
    "        'state': lambda x: x.value_counts().get('open', 0),\n",
    "        'closed_at': lambda x: x.notnull().sum()\n",
    "    }).rename(columns={'state': 'open_issues', 'closed_at': 'closed_issues'})\n",
    "\n",
    "    # Calculate cumulative sums\n",
    "    monthly_data['cumulative_open'] = monthly_data['open_issues'].cumsum()\n",
    "    monthly_data['cumulative_closed'] = monthly_data['closed_issues'].cumsum()\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "def plot_open_and_closed(data, data_type, log_scale=False):\n",
    "\n",
    "    monthly_data = calculate_open_and_closed(data)\n",
    "\n",
    "    # Define the number of months per x-axis label\n",
    "    months_per_label = 3  # change this to any number of months that should be one label\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(monthly_data.index, monthly_data['cumulative_open'], label='Cumulative Open', marker='o', linestyle='-')\n",
    "    plt.plot(monthly_data.index, monthly_data['cumulative_closed'], label='Cumulative Closed', marker='x', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Cumulative Open and Closed {data_type}s Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'Number of {data_type}s')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if(log_scale):\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel(f'Number of {data_type}s (Log Scale)')\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))  # Set major ticks to every custom number of months\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_open_and_closed(issues_data, 'Issue', log_scale=True)\n",
    "plot_open_and_closed(pull_requests_data, 'Pull Request', log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_open_and_closed(data):\n",
    "    plot_data = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        closed_at = entry['closed_at']\n",
    "        if closed_at:\n",
    "            closed_at = datetime.strptime(closed_at, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        plot_data.append((created_at, closed_at, state))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(plot_data, columns=['created_at', 'closed_at', 'state'])\n",
    "\n",
    "    # Resample data by month\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.resample('M').apply({\n",
    "        'state': lambda x: x.value_counts().get('open', 0),\n",
    "        'closed_at': lambda x: x.notnull().sum()\n",
    "    }).rename(columns={'state': 'open_issues', 'closed_at': 'closed_issues'})\n",
    "\n",
    "    # Calculate cumulative sums\n",
    "    monthly_data['cumulative_open'] = monthly_data['open_issues'].cumsum()\n",
    "    monthly_data['cumulative_closed'] = monthly_data['closed_issues'].cumsum()\n",
    "\n",
    "    return monthly_data\n",
    "\n",
    "def plot_open_and_closed(data, data_type, log_scale=False, export_format=EXPORT_FORMAT):\n",
    "    # Generate data for plotting\n",
    "    monthly_data = calculate_open_and_closed(data)\n",
    "\n",
    "    # Export the data if specified\n",
    "    export_df(monthly_data, f'{data_type.lower()}_open_and_closed', export_format)\n",
    "\n",
    "    # Plot the data (for visualization, if needed)\n",
    "    plt.figure(figsize=(18, 8))  # Make the plot wider\n",
    "    plt.plot(monthly_data.index, monthly_data['cumulative_open'], label='Cumulative Open', marker='o', linestyle='-')\n",
    "    plt.plot(monthly_data.index, monthly_data['cumulative_closed'], label='Cumulative Closed', marker='x', linestyle='-')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Cumulative Open and Closed {data_type}s Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'Number of {data_type}s')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel(f'Number of {data_type}s (Log Scale)')\n",
    "\n",
    "    # Improve the date formatting and set custom granularity\n",
    "    months_per_label = 3  # Adjust as desired\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS:\n",
    "        if not log_scale: plt.savefig(f\"{data_type.lower()}_open_and_closed.png\")\n",
    "        else: plt.savefig(f\"{data_type.lower()}_open_and_closed_log.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_open_and_closed(issues_data, 'Issue', log_scale=True, export_format=EXPORT_FORMAT)\n",
    "plot_open_and_closed(pull_requests_data, 'Pull Request', log_scale=True, export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data\n",
    "def extract_age(data):\n",
    "    ages = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        if state == 'open':\n",
    "            age = (datetime.now() - created_at).days\n",
    "            ages.append((created_at, age))\n",
    "    return ages\n",
    "\n",
    "def plot_age(data, data_type):\n",
    "    ages = extract_age(data)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_age = pd.DataFrame(ages, columns=['created_at', 'age'])\n",
    "\n",
    "    # Convert dates to datetime format\n",
    "    df_age['created_at'] = pd.to_datetime(df_age['created_at'])\n",
    "\n",
    "    # Sort by creation date\n",
    "    df_age.sort_values('created_at', inplace=True)\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(df_age['age'], bins=20, edgecolor='black')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Distribution of Ages of Open {data_type}s')\n",
    "    plt.xlabel(f'Age of Open {data_type}s (days)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_age(issues_data, 'Issue')\n",
    "plot_age(pull_requests_data, 'Pull Request')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_age(data):\n",
    "    ages = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        if state == 'open':\n",
    "            age = (datetime.now() - created_at).days\n",
    "            ages.append((created_at, age))\n",
    "    return ages\n",
    "\n",
    "def plot_age(data, data_type, export_format=EXPORT_FORMAT):\n",
    "    # Extract ages of open entries\n",
    "    ages = extract_age(data)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_age = pd.DataFrame(ages, columns=['created_at', 'age'])\n",
    "    df_age['created_at'] = pd.to_datetime(df_age['created_at'])\n",
    "\n",
    "    # Export the data if specified\n",
    "    export_df(df_age, f'{data_type.lower()}_ages', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(df_age['age'], bins=20, edgecolor='black')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Distribution of Ages of Open {data_type}s')\n",
    "    plt.xlabel(f'Age of Open {data_type}s (days)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{data_type.lower()}_ages.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_age(issues_data, 'Issue', export_format=EXPORT_FORMAT)\n",
    "plot_age(pull_requests_data, 'Pull Request', export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data\n",
    "def extract_open_counts(data):\n",
    "    open_counts = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        if state == 'open':\n",
    "            open_counts.append(created_at)\n",
    "    return open_counts\n",
    "\n",
    "def plot_open_counts(data, data_type):\n",
    "    open_counts = extract_open_counts(data)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_open_counts = pd.DataFrame(open_counts, columns=['created_at'])\n",
    "\n",
    "    # Convert dates to datetime format\n",
    "    df_open_counts['created_at'] = pd.to_datetime(df_open_counts['created_at'])\n",
    "\n",
    "    # Group by month and count the number of open issues/PRs\n",
    "    monthly_open_counts = df_open_counts.groupby(df_open_counts['created_at'].dt.to_period('M')).size().reset_index(name='count')\n",
    "    monthly_open_counts['created_at'] = monthly_open_counts['created_at'].dt.to_timestamp()\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.bar(monthly_open_counts['created_at'], monthly_open_counts['count'], width=20, align='center')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Number of Open {data_type}s Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel(f'Number of Open {data_type}s')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Ensure y-axis has only integer values\n",
    "    plt.gca().yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Improve the date formatting\n",
    "    months_per_label = 3  # Adjust the interval as needed\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_open_counts(issues_data, 'Issue')\n",
    "plot_open_counts(pull_requests_data, 'Pull Request')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_open_counts(data):\n",
    "    open_counts = []\n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        state = entry['state']\n",
    "        if state == 'open':\n",
    "            open_counts.append(created_at)\n",
    "    return open_counts\n",
    "\n",
    "def plot_open_counts(data, data_type, export_format=EXPORT_FORMAT):\n",
    "    # Extract open item dates\n",
    "    open_counts = extract_open_counts(data)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_open_counts = pd.DataFrame(open_counts, columns=['created_at'])\n",
    "    df_open_counts['created_at'] = pd.to_datetime(df_open_counts['created_at'])\n",
    "\n",
    "    # Group by month and count the open items\n",
    "    monthly_open_counts = df_open_counts.groupby(df_open_counts['created_at'].dt.to_period('M')).size().reset_index(name='count')\n",
    "    monthly_open_counts['created_at'] = monthly_open_counts['created_at'].dt.to_timestamp()\n",
    "\n",
    "    # Export the data if specified\n",
    "    export_df(monthly_open_counts, f'{data_type.lower()}_open_counts', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.bar(monthly_open_counts['created_at'], monthly_open_counts['count'], width=20, align='center')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'Number of Open {data_type}s Over Time')\n",
    "    plt.xlabel('Creation Date')\n",
    "    plt.ylabel(f'Number of Open {data_type}s')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Ensure y-axis has only integer values\n",
    "    plt.gca().yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Improve the date formatting\n",
    "    months_per_label = 3  # Adjust as needed\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{data_type.lower()}_open_counts.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_open_counts(issues_data, 'Issue', export_format=EXPORT_FORMAT)\n",
    "plot_open_counts(pull_requests_data, 'Pull Request', export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract commit data from pull requests\n",
    "# def extract_commit_data_from_prs(pull_requests_data):\n",
    "#     commit_info = []\n",
    "#     for pr in pull_requests_data:\n",
    "#         for commit in pr['commits']:\n",
    "#             commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#             changes = sum(file['additions'] + file['deletions'] for file in pr['files'])\n",
    "#             commit_info.append((commit_date, changes))\n",
    "#     return commit_info\n",
    "\n",
    "# # Plot commit frequency and size\n",
    "# def plot_commit_metrics(commit_info):\n",
    "#     df = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     df['commit_date'] = pd.to_datetime(df['commit_date'])\n",
    "\n",
    "#     # Group by day\n",
    "#     daily_commits = df.groupby(df['commit_date'].dt.date).size()\n",
    "#     daily_changes = df.groupby(df['commit_date'].dt.date)['changes'].mean()\n",
    "\n",
    "#     fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "#     # Bar plot for daily commits\n",
    "#     ax1.bar(daily_commits.index, daily_commits.values, width=0.8, alpha=0.6, label='Daily Commits', color='blue')\n",
    "#     ax1.set_xlabel('Date')\n",
    "#     ax1.set_ylabel('Number of Commits')\n",
    "#     ax1.legend(loc='upper left')\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     # Secondary axis for average commit size\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(daily_changes.index, daily_changes.values, linestyle='-', color='red', label='Average Commit Size')\n",
    "#     ax2.set_ylabel('Average Commit Size (lines)')\n",
    "#     ax2.legend(loc='upper right')\n",
    "\n",
    "#     plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage:\n",
    "# commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "# plot_commit_metrics(commit_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_commit_frequency_and_size(data, months_per_label=3):\n",
    "#     commits = []\n",
    "\n",
    "#     for pr in data:\n",
    "#         for commit in pr['commits']:\n",
    "#             commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#             commit_size = pr['additions'] + pr['deletions']\n",
    "#             commits.append((commit_date, commit_size))\n",
    "\n",
    "#     # Create a DataFrame\n",
    "#     df_commits = pd.DataFrame(commits, columns=['date', 'size'])\n",
    "\n",
    "#     # Convert dates to datetime format\n",
    "#     df_commits['date'] = pd.to_datetime(df_commits['date'])\n",
    "\n",
    "#     # Resample data by day and calculate the number of commits and average commit size\n",
    "#     df_daily = df_commits.resample('D', on='date').agg({'size': ['count', 'mean']}).reset_index()\n",
    "#     df_daily.columns = ['date', 'commit_count', 'avg_commit_size']\n",
    "\n",
    "#     # Plotting the data\n",
    "#     fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "#     ax1.plot(df_daily['date'], df_daily['commit_count'], color='blue', label='Daily Commits')\n",
    "#     ax1.set_xlabel('Date')\n",
    "#     ax1.set_ylabel('Number of Commits', color='blue')\n",
    "#     ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(df_daily['date'], df_daily['avg_commit_size'], color='red', label='Average Commit Size')\n",
    "#     ax2.set_ylabel('Average Commit Size (lines)', color='red')\n",
    "#     ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "\n",
    "#     # Improve the date formatting and set custom granularity\n",
    "#     plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "#     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "#     plt.gcf().autofmt_xdate()\n",
    "\n",
    "#     fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_commit_frequency_and_size(pull_requests_data, months_per_label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commit_frequency_and_size(data, log_scale=False):\n",
    "    # Extract relevant data\n",
    "    commits = []\n",
    "    for pr in data:\n",
    "        for commit in pr['commits']:\n",
    "            date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            size = pr['additions'] + pr['deletions']\n",
    "            commits.append((date, size))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_commits = pd.DataFrame(commits, columns=['date', 'size'])\n",
    "    \n",
    "    # Resample by month\n",
    "    df_commits.set_index('date', inplace=True)\n",
    "    df_monthly = df_commits.resample('M').agg({'size': ['mean', 'count']})\n",
    "    df_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    df_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Calculate rolling averages to smooth data\n",
    "    df_monthly['monthly_commits'] = df_monthly['monthly_commits'].rolling(window=3, min_periods=1).mean()\n",
    "    df_monthly['avg_size'] = df_monthly['avg_size'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Plotting the data\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax1.bar(df_monthly.index, df_monthly['monthly_commits'], width=20, color='blue', alpha=0.5, label='Monthly Commits')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    # ax2.plot(df_monthly.index, df_monthly['avg_size'], color='red', marker='o', label='Average Commit Size')\n",
    "    ax2.plot(df_monthly.index, df_monthly['avg_size'], color='red', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='red')\n",
    "    if(log_scale): ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your pull requests data\n",
    "plot_commit_frequency_and_size(pull_requests_data)\n",
    "plot_commit_frequency_and_size(pull_requests_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_commit_frequency_and_size(data, log_scale=False, export_format=EXPORT_FORMAT):\n",
    "    # Extract relevant data\n",
    "    commits = []\n",
    "    for pr in data:\n",
    "        for commit in pr['commits']:\n",
    "            date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            size = pr['additions'] + pr['deletions']\n",
    "            commits.append((date, size))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_commits = pd.DataFrame(commits, columns=['date', 'size'])\n",
    "    \n",
    "    # Resample by month\n",
    "    df_commits.set_index('date', inplace=True)\n",
    "    df_monthly = df_commits.resample('M').agg({'size': ['mean', 'count']})\n",
    "    df_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    df_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    df_monthly['monthly_commits'] = df_monthly['monthly_commits'].rolling(window=3, min_periods=1).mean()\n",
    "    df_monthly['avg_size'] = df_monthly['avg_size'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Export the data if specified\n",
    "    export_df(df_monthly, f'commit_frequency_and_size', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax1.bar(df_monthly.index, df_monthly['monthly_commits'], width=20, color='blue', alpha=0.5, label='Monthly Commits')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_monthly.index, df_monthly['avg_size'], color='red', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='red')\n",
    "    if log_scale:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add title and layout adjustments\n",
    "    plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if EXPORT_PNGS:\n",
    "        if not log_scale: plt.savefig(f\"commit_frequency_and_size.png\")\n",
    "        else: plt.savefig(f\"commit_frequency_and_sized_log.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_commit_frequency_and_size(pull_requests_data, log_scale=False, export_format=EXPORT_FORMAT)\n",
    "plot_commit_frequency_and_size(pull_requests_data, log_scale=True, export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commit_frequency_and_size(data, log_scale=False):\n",
    "    # Extract relevant data\n",
    "    commits = []\n",
    "    for pr in data:\n",
    "        for commit in pr['commits']:\n",
    "            date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            size = pr['additions'] + pr['deletions']\n",
    "            commits.append((date, size))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_commits = pd.DataFrame(commits, columns=['date', 'size'])\n",
    "    \n",
    "    # Sort by date and filter out initial outliers\n",
    "    df_commits.sort_values('date', inplace=True)\n",
    "    initial_commits = df_commits.head(20)  # Consider the first 20 commits\n",
    "    remaining_commits = df_commits.iloc[20:]  # Rest of the commits\n",
    "    \n",
    "    # Calculate IQR for initial commits\n",
    "    Q1 = initial_commits['size'].quantile(0.25)\n",
    "    Q3 = initial_commits['size'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter_outliers = (initial_commits['size'] >= (Q1 - 1.5 * IQR)) & (initial_commits['size'] <= (Q3 + 1.5 * IQR))\n",
    "    filtered_initial_commits = initial_commits[filter_outliers]\n",
    "    \n",
    "    # Combine filtered initial commits with the rest\n",
    "    df_commits_filtered = pd.concat([filtered_initial_commits, remaining_commits])\n",
    "    \n",
    "    # Resample by month\n",
    "    df_commits_filtered.set_index('date', inplace=True)\n",
    "    df_monthly = df_commits_filtered.resample('M').agg({'size': ['mean', 'count']})\n",
    "    df_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    df_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Calculate rolling averages to smooth data\n",
    "    df_monthly['monthly_commits'] = df_monthly['monthly_commits'].rolling(window=3, min_periods=1).mean()\n",
    "    df_monthly['avg_size'] = df_monthly['avg_size'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Plotting the data\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax1.bar(df_monthly.index, df_monthly['monthly_commits'], width=20, color='blue', alpha=0.5, label='Monthly Commits')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_monthly.index, df_monthly['avg_size'], color='red', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='red')\n",
    "    if log_scale: ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your pull requests data\n",
    "plot_commit_frequency_and_size(pull_requests_data)\n",
    "plot_commit_frequency_and_size(pull_requests_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_commit_frequency_and_size(data, log_scale=False, export_format=EXPORT_FORMAT):\n",
    "    # Extract relevant data\n",
    "    commits = []\n",
    "    for pr in data:\n",
    "        for commit in pr['commits']:\n",
    "            date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            size = pr['additions'] + pr['deletions']\n",
    "            commits.append((date, size))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_commits = pd.DataFrame(commits, columns=['date', 'size'])\n",
    "    \n",
    "    # Sort by date and filter out initial outliers\n",
    "    df_commits.sort_values('date', inplace=True)\n",
    "    initial_commits = df_commits.head(20)\n",
    "    remaining_commits = df_commits.iloc[20:]\n",
    "\n",
    "    # Calculate IQR for initial commits\n",
    "    Q1 = initial_commits['size'].quantile(0.25)\n",
    "    Q3 = initial_commits['size'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter_outliers = (initial_commits['size'] >= (Q1 - 1.5 * IQR)) & (initial_commits['size'] <= (Q3 + 1.5 * IQR))\n",
    "    filtered_initial_commits = initial_commits[filter_outliers]\n",
    "    \n",
    "    # Combine filtered initial commits with the rest\n",
    "    df_commits_filtered = pd.concat([filtered_initial_commits, remaining_commits])\n",
    "    \n",
    "    # Resample by month\n",
    "    df_commits_filtered.set_index('date', inplace=True)\n",
    "    df_monthly = df_commits_filtered.resample('M').agg({'size': ['mean', 'count']})\n",
    "    df_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    df_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    df_monthly['monthly_commits'] = df_monthly['monthly_commits'].rolling(window=3, min_periods=1).mean()\n",
    "    df_monthly['avg_size'] = df_monthly['avg_size'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    # Export the data if specified\n",
    "    export_df(df_monthly, f'commit_frequency_and_size_filtered', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax1.bar(df_monthly.index, df_monthly['monthly_commits'], width=20, color='blue', alpha=0.5, label='Monthly Commits')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_monthly.index, df_monthly['avg_size'], color='red', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='red')\n",
    "    if log_scale:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add title and layout adjustments\n",
    "    plt.title('Commit Frequency and Average Commit Size Over Time (Filtered)')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if EXPORT_PNGS:\n",
    "        if not log_scale: plt.savefig(f\"commit_frequency_and_size_filtered.png\")\n",
    "        else: plt.savefig(f\"commit_frequency_and_size_filtered_log.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Example calls with data export\n",
    "plot_commit_frequency_and_size(pull_requests_data, log_scale=False, export_format=EXPORT_FORMAT)\n",
    "plot_commit_frequency_and_size(pull_requests_data, log_scale=True, export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Analysis\n",
    "\n",
    "### Disclaimer: This is specific to the vaadin/flow repo and won't for others as the label names often differ between different repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(data):\n",
    "    labels = {}\n",
    "    for entry in data:\n",
    "        if(entry['labels']):\n",
    "            for label in entry['labels']:\n",
    "                labels[label] = labels.get(label, 0) + 1\n",
    "    return labels\n",
    "\n",
    "labels = collect_labels(issues_data)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label groups and merge 'enhancement' and 'feature request'\n",
    "label_groups = {\n",
    "    'Requests': ['bug', 'enhancement', 'feature request', 'documentation', 'question', 'help wanted'],\n",
    "    'Impact': ['Impact: High', 'Impact: Low'],\n",
    "    'Severity': ['Severity: Blocker', 'Severity: Minor'],\n",
    "    'Code Quality': ['code quality', 'refactor', 'breaking change']\n",
    "}\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_label_data(data, label_groups):\n",
    "    label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        labels = entry['labels']\n",
    "        for label in labels:\n",
    "            for group, labels_list in label_groups.items():\n",
    "                if label in labels_list:\n",
    "                    # Merge 'enhancement' and 'feature request' labels\n",
    "                    if label in ['enhancement', 'feature request']:\n",
    "                        label = 'enhancement'\n",
    "                    if label in ['documentation', 'question', 'help wanted']:\n",
    "                        label = 'support/documentation'\n",
    "                    label_data[group].append((created_at, label))\n",
    "    \n",
    "    return label_data\n",
    "\n",
    "def plot_label_group(data, label_group, group_name, months_per_label=3):\n",
    "    df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    \n",
    "    # Convert dates to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "    # Resample data by month\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.groupby([pd.Grouper(freq='M'), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Plotting the data\n",
    "    monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'{group_name} Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=group_name)\n",
    "\n",
    "    # Improve the date formatting\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Extract label data\n",
    "label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# Plot each label group\n",
    "for group_name, data in label_data.items():\n",
    "    plot_label_group(data, label_groups[group_name], group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Define label groups and merge 'enhancement' and 'feature request'\n",
    "label_groups = {\n",
    "    'Requests': ['bug', 'enhancement', 'feature request', 'documentation', 'question', 'help wanted'],\n",
    "    'Impact': ['Impact: High', 'Impact: Low'],\n",
    "    'Severity': ['Severity: Blocker', 'Severity: Minor'],\n",
    "    'Code Quality': ['code quality', 'refactor', 'breaking change']\n",
    "}\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_label_data(data, label_groups):\n",
    "    label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        labels = entry['labels']\n",
    "        for label in labels:\n",
    "            for group, labels_list in label_groups.items():\n",
    "                if label in labels_list:\n",
    "                    # Merge 'enhancement' and 'feature request' labels\n",
    "                    if label in ['enhancement', 'feature request']:\n",
    "                        label = 'enhancement'\n",
    "                    if label in ['documentation', 'question', 'help wanted']:\n",
    "                        label = 'support/documentation'\n",
    "                    label_data[group].append((created_at, label))\n",
    "    \n",
    "    return label_data\n",
    "\n",
    "def plot_label_group(data, label_group, group_name, months_per_label=3, export_format=EXPORT_FORMAT):\n",
    "    df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "    # Resample data by month and count occurrences of each label\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.groupby([pd.Grouper(freq='M'), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Export the data if specified\n",
    "    export_df(monthly_data, f'{group_name.lower().replace(\" \", \"_\")}_label_data', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'{group_name} Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=group_name)\n",
    "\n",
    "    # Improve the date formatting\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    if EXPORT_PNGS: plt.savefig(f\"{group_name.lower().replace(' ', '_')}_label_data.png\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Extract label data\n",
    "label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# Plot and export each label group\n",
    "for group_name, data in label_data.items():\n",
    "    plot_label_group(data, label_groups[group_name], group_name, export_format=EXPORT_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out what caused the spike in Bugs in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert issues data to DataFrame\n",
    "# issues_df = pd.DataFrame(issues_data)\n",
    "\n",
    "# # Convert 'created_at' to datetime\n",
    "# issues_df['created_at'] = pd.to_datetime(issues_df['created_at'])\n",
    "\n",
    "# # Filter issues by date range\n",
    "# start_date = '2019-06-16' # 2 weeks before the time interval of the spike started\n",
    "# end_date = '2019-09-30' # end of the time interval of the spike\n",
    "# filtered_issues = issues_df[(issues_df['created_at'] >= start_date) & (issues_df['created_at'] < end_date)]\n",
    "\n",
    "# # Further filter by 'bug' label\n",
    "# bug_issues = filtered_issues[filtered_issues['labels'].apply(lambda x: 'bug' in x)]\n",
    "\n",
    "# # Print titles and descriptions of bug issues in the specified date range\n",
    "# for index, issue in bug_issues.iterrows():\n",
    "#     print(f\"Issue #{issue['number']}: {issue['title']}\")\n",
    "#     print(f\"Description: {issue['description']}\\n\")\n",
    "\n",
    "# # Analyze the corresponding pull requests\n",
    "# pr_numbers = [pr['number'] for pr in pull_requests_data if any(fixes_issue in issue['description'] for fixes_issue in bug_issues['number'].astype(str))]\n",
    "\n",
    "# # Print the related pull requests\n",
    "# for pr in pull_requests_data:\n",
    "#     if pr['number'] in pr_numbers:\n",
    "#         print(f\"PR #{pr['number']}: {pr['title']}\")\n",
    "#         print(f\"Description: {pr['description']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the time interval\n",
    "# start_date = datetime(2019, 7, 16)\n",
    "# end_date = datetime(2019, 8, 31)\n",
    "\n",
    "# # Convert issues data to DataFrame\n",
    "# issues_df = pd.DataFrame(issues_data)\n",
    "# issues_df['created_at'] = pd.to_datetime(issues_df['created_at'])\n",
    "# issues_df['closed_at'] = pd.to_datetime(issues_df['closed_at'])\n",
    "\n",
    "# # Convert pull requests data to DataFrame\n",
    "# prs_df = pd.DataFrame(pull_requests_data)\n",
    "# prs_df['created_at'] = pd.to_datetime(prs_df['created_at'])\n",
    "# prs_df['merged_at'] = pd.to_datetime(prs_df['merged_at'])\n",
    "# prs_df['closed_at'] = pd.to_datetime(prs_df['closed_at'])\n",
    "\n",
    "# # Filter data by the defined interval\n",
    "# filtered_issues = issues_df[(issues_df['created_at'] >= start_date) & (issues_df['created_at'] <= end_date)]\n",
    "# filtered_prs = prs_df[(prs_df['created_at'] >= start_date) & (prs_df['created_at'] <= end_date)]\n",
    "\n",
    "# def plot_counts_over_time(issues_df, prs_df):\n",
    "#     plt.figure(figsize=(18, 8))\n",
    "    \n",
    "#     # Plot issues\n",
    "#     issues_counts = issues_df.groupby(issues_df['created_at'].dt.date).size()\n",
    "#     plt.plot(issues_counts.index, issues_counts.values, linestyle='-', label='Issues')\n",
    "    \n",
    "#     # Plot pull requests\n",
    "#     prs_counts = prs_df.groupby(prs_df['created_at'].dt.date).size()\n",
    "#     plt.plot(prs_counts.index, prs_counts.values, linestyle='-', label='Pull Requests')\n",
    "    \n",
    "#     plt.title('Number of Issues and Pull Requests Over Time')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_counts_over_time(filtered_issues, filtered_prs)\n",
    "\n",
    "# def plot_turnaround_time(issues_df):\n",
    "#     turnaround_times = issues_df[issues_df['closed_at'].notnull()].copy()\n",
    "#     turnaround_times['turnaround_time'] = (turnaround_times['closed_at'] - turnaround_times['created_at']).dt.total_seconds() / 3600  # convert to hours\n",
    "\n",
    "#     plt.figure(figsize=(18, 8))\n",
    "#     plt.plot(turnaround_times['created_at'], turnaround_times['turnaround_time'], linestyle='-', label='Turnaround Time')\n",
    "\n",
    "#     plt.title('Turnaround Time for Issues Over Time')\n",
    "#     plt.xlabel('Creation Date')\n",
    "#     plt.ylabel('Turnaround Time (hours)')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_turnaround_time(filtered_issues)\n",
    "\n",
    "# def plot_open_closed_ratio(issues_df):\n",
    "#     issues_df['state'] = issues_df['closed_at'].apply(lambda x: 'closed' if pd.notnull(x) else 'open')\n",
    "#     daily_open_issues = issues_df[issues_df['state'] == 'open'].groupby(issues_df['created_at'].dt.date).size().cumsum()\n",
    "#     daily_closed_issues = issues_df[issues_df['state'] == 'closed'].groupby(issues_df['created_at'].dt.date).size().cumsum()\n",
    "\n",
    "#     plt.figure(figsize=(18, 8))\n",
    "#     plt.plot(daily_open_issues.index, daily_open_issues.values, linestyle='-', label='Open Issues')\n",
    "#     plt.plot(daily_closed_issues.index, daily_closed_issues.values, linestyle='-', label='Closed Issues')\n",
    "\n",
    "#     plt.title('Open vs. Closed Issues Over Time')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Cumulative Count')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_open_closed_ratio(filtered_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commit_data_from_prs(pull_requests_data):\n",
    "    commit_info = []\n",
    "    for pr in pull_requests_data:\n",
    "        for commit in pr['commits']:\n",
    "            commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            # Calculate the changes as the sum of additions and deletions\n",
    "            changes = sum(file['additions'] + file['deletions'] for file in pr['files'])\n",
    "            commit_info.append((commit_date, changes))\n",
    "    return commit_info\n",
    "\n",
    "# Example usage:\n",
    "commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# Filter commits by a specific time interval (if needed)\n",
    "start_date = datetime(2019, 7, 16)\n",
    "end_date = datetime(2019, 8, 31)\n",
    "filtered_commits = [commit for commit in commit_info if start_date <= commit[0] <= end_date]\n",
    "\n",
    "def plot_commit_metrics(commit_info):\n",
    "    df = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df['commit_date'] = pd.to_datetime(df['commit_date'])\n",
    "\n",
    "    # Group by day\n",
    "    daily_commits = df.groupby(df['commit_date'].dt.date).size()\n",
    "    daily_changes = df.groupby(df['commit_date'].dt.date)['changes'].mean()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    # Bar plot for daily commits\n",
    "    ax1.bar(daily_commits.index, daily_commits.values, width=0.8, alpha=0.6, label='Daily Commits', color='blue')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Secondary axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(daily_changes.index, daily_changes.values, marker='o', linestyle='-', color='red', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.title('Commit Frequency and Average Commit Size Over Time')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_commit_metrics(filtered_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define label groups and merge 'enhancement' and 'feature request'\n",
    "# label_groups = {'Requests': ['enhancement', 'feature request', 'bug']}\n",
    "\n",
    "# # Extract relevant data\n",
    "# def extract_label_data(data, label_groups):\n",
    "#     label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "#     for entry in data:\n",
    "#         created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#         labels = entry['labels']\n",
    "#         for label in labels:\n",
    "#             for group, labels_list in label_groups.items():\n",
    "#                 if label in labels_list:\n",
    "#                     # Merge 'enhancement' and 'feature request' labels\n",
    "#                     if label in ['enhancement', 'feature request']:\n",
    "#                         label = 'enhancement/feature request'\n",
    "#                     label_data[group].append((created_at, label))\n",
    "    \n",
    "#     return label_data\n",
    "\n",
    "# def plot_label_group(data, label_group, group_name, months_per_label=3):\n",
    "#     df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    \n",
    "#     # Convert dates to datetime format\n",
    "#     df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "#     # Resample data by month\n",
    "#     df.set_index('created_at', inplace=True)\n",
    "#     monthly_data = df.groupby([pd.Grouper(freq='M'), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "#     # Plotting the data\n",
    "#     monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "#     # Formatting the plot\n",
    "#     plt.title(f'{group_name} Over Time')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend(title=group_name)\n",
    "\n",
    "#     # Improve the date formatting\n",
    "#     plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "#     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "#     plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "#     # Display the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract label data\n",
    "# label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# # Extract commit data\n",
    "# def extract_commit_data_from_prs(pull_requests_data):\n",
    "#     commit_info = []\n",
    "#     for pr in pull_requests_data:\n",
    "#         for commit in pr['commits']:\n",
    "#             commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#             # Calculate the changes as the sum of additions and deletions\n",
    "#             changes = pr['additions'] + pr['deletions']\n",
    "#             commit_info.append((commit_date, changes))\n",
    "#     return commit_info\n",
    "\n",
    "# commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# # Filter commits by date and size to remove initial outliers\n",
    "# def filter_initial_outliers(commit_info, threshold=0.01):\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     initial_period_end = df_commits['commit_date'].min() + pd.Timedelta(days=30)\n",
    "#     initial_commits = df_commits[df_commits['commit_date'] <= initial_period_end]\n",
    "#     filtered_commits = df_commits[df_commits['commit_date'] > initial_period_end]\n",
    "\n",
    "#     if not initial_commits.empty:\n",
    "#         upper_limit = initial_commits['changes'].quantile(1 - threshold)\n",
    "#         filtered_initial_commits = initial_commits[initial_commits['changes'] <= upper_limit]\n",
    "#         return pd.concat([filtered_initial_commits, filtered_commits])\n",
    "#     return df_commits\n",
    "\n",
    "# filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "\n",
    "# def plot_commit_metrics_with_bugs(commit_info, bug_issues, start_date, end_date, log_scale=False):\n",
    "#     # Convert commit info to DataFrame\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "    \n",
    "#     # Filter by date\n",
    "#     df_commits = df_commits[(df_commits['commit_date'] >= start_date) & (df_commits['commit_date'] <= end_date)]\n",
    "\n",
    "#     # Group by day\n",
    "#     daily_commits = df_commits.groupby(df_commits['commit_date'].dt.date).size()\n",
    "#     daily_changes = df_commits.groupby(df_commits['commit_date'].dt.date)['changes'].mean()\n",
    "    \n",
    "#     # Convert bug issues to DataFrame and filter by date\n",
    "#     df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "#     df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "#     df_bugs = df_bugs[(df_bugs['created_at'] >= start_date) & (df_bugs['created_at'] <= end_date)]\n",
    "    \n",
    "#     # Group bug issues by day\n",
    "#     daily_bugs = df_bugs.groupby(df_bugs['created_at'].dt.date).size()\n",
    "    \n",
    "#     fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "#     # Bar plot for daily commits\n",
    "#     ax1.bar(daily_commits.index, daily_commits.values, width=0.8, alpha=0.6, label='Daily Commits', color='blue')\n",
    "#     ax1.set_xlabel('Date')\n",
    "#     ax1.set_ylabel('Number of Commits')\n",
    "#     ax1.legend(loc='upper left')\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     # Plot bug issues\n",
    "#     ax1.plot(daily_bugs.index, daily_bugs.values, 'g--', marker='x', label='Bug Issues')\n",
    "    \n",
    "#     # Secondary axis for average commit size\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(daily_changes.index, daily_changes.values, marker='o', linestyle='-', color='red', label='Average Commit Size')\n",
    "#     ax2.set_ylabel('Average Commit Size (lines)')\n",
    "#     if log_scale: ax2.set_yscale('log')\n",
    "#     ax2.legend(loc='upper right')\n",
    "\n",
    "#     plt.title('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "#     plt.show()\n",
    "\n",
    "# # Define the time interval for the spike\n",
    "# start_date = datetime(2019, 7, 16)\n",
    "# end_date = datetime(2019, 8, 31)\n",
    "\n",
    "# # Filter commits and bug issues\n",
    "# filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "# bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "# filtered_bug_issues = [issue for issue in bug_issues if start_date <= datetime.strptime(issue['created_at'], \"%Y-%m-%dT%H:%M:%S\") <= end_date]\n",
    "\n",
    "# # Plot commit metrics with bug issues\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, filtered_bug_issues, start_date, end_date)\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, filtered_bug_issues, start_date, end_date, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label groups and merge 'enhancement' and 'feature request'\n",
    "label_groups = {'Requests': ['enhancement', 'feature request', 'bug', 'documentation', 'question', 'help wanted']}\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_label_data(data, label_groups):\n",
    "    label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "    for entry in data:\n",
    "        created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        labels = entry['labels']\n",
    "        for label in labels:\n",
    "            for group, labels_list in label_groups.items():\n",
    "                if label in labels_list:\n",
    "                    # Merge 'enhancement' and 'feature request' labels\n",
    "                    if label in ['enhancement', 'feature request']:\n",
    "                        label = 'enhancement'\n",
    "                    if label in ['documentation', 'question', 'help wanted']:\n",
    "                        label = 'support/documentation'\n",
    "                    label_data[group].append((created_at, label))\n",
    "    \n",
    "    return label_data\n",
    "\n",
    "def plot_label_group(data, label_group, group_name, months_per_label=3):\n",
    "    df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    \n",
    "    # Convert dates to datetime format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "    # Resample data by month\n",
    "    df.set_index('created_at', inplace=True)\n",
    "    monthly_data = df.groupby([pd.Grouper(freq='M'), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Plotting the data\n",
    "    monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.title(f'{group_name} Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=group_name)\n",
    "\n",
    "    # Improve the date formatting\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Extract label data\n",
    "label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# Extract commit data\n",
    "def extract_commit_data_from_prs(pull_requests_data):\n",
    "    commit_info = []\n",
    "    for pr in pull_requests_data:\n",
    "        for commit in pr['commits']:\n",
    "            commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            # Calculate the changes as the sum of additions and deletions\n",
    "            changes = pr['additions'] + pr['deletions']\n",
    "            commit_info.append((commit_date, changes))\n",
    "    return commit_info\n",
    "\n",
    "commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# Filter commits by date and size to remove initial outliers\n",
    "def filter_initial_outliers(commit_info, threshold=0.01):\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    initial_period_end = df_commits['commit_date'].min() + pd.Timedelta(days=30)\n",
    "    initial_commits = df_commits[df_commits['commit_date'] <= initial_period_end]\n",
    "    filtered_commits = df_commits[df_commits['commit_date'] > initial_period_end]\n",
    "\n",
    "    if not initial_commits.empty:\n",
    "        upper_limit = initial_commits['changes'].quantile(1 - threshold)\n",
    "        filtered_initial_commits = initial_commits[initial_commits['changes'] <= upper_limit]\n",
    "        return pd.concat([filtered_initial_commits, filtered_commits])\n",
    "    return df_commits\n",
    "\n",
    "filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "\n",
    "def plot_commit_metrics_with_bugs(commit_info, bug_issues, start_date, end_date, log_scale=False):\n",
    "    # Convert commit info to DataFrame\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "    \n",
    "    # Filter by date\n",
    "    df_commits = df_commits[(df_commits['commit_date'] >= start_date) & (df_commits['commit_date'] <= end_date)]\n",
    "\n",
    "    # Group by day\n",
    "    daily_commits = df_commits.groupby(df_commits['commit_date'].dt.date).size()\n",
    "    daily_changes = df_commits.groupby(df_commits['commit_date'].dt.date)['changes'].mean()\n",
    "    \n",
    "    # Convert bug issues to DataFrame and filter by date\n",
    "    df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "    df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "    df_bugs = df_bugs[(df_bugs['created_at'] >= start_date) & (df_bugs['created_at'] <= end_date)]\n",
    "    \n",
    "    # Group bug issues by day\n",
    "    daily_bugs = df_bugs.groupby(df_bugs['created_at'].dt.date).size()\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    # Bar plot for daily commits\n",
    "    ax1.bar(daily_commits.index, daily_commits.values, width=0.8, alpha=0.6, label='Daily Commits', color='green')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Secondary axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(daily_changes.index, daily_changes.values, marker='o', linestyle='-', color='blue', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='blue')\n",
    "    if log_scale: ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Third axis for bug issues\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.plot(daily_bugs.index, daily_bugs.values, 'r-', marker='x', label='Bug Issues')\n",
    "    ax3.set_ylabel('Bug Issues', color='red')\n",
    "    ax3.tick_params(axis='y', labelcolor='red')\n",
    "    # Set the y-scale for bug issues to display integers only\n",
    "    ax3.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Add title and legend\n",
    "    fig.suptitle('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Define the time interval for the spike\n",
    "start_date = datetime(2019, 7, 16)\n",
    "end_date = datetime(2019, 8, 31)\n",
    "\n",
    "# Filter commits and bug issues\n",
    "filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "filtered_bug_issues = [issue for issue in bug_issues if start_date <= datetime.strptime(issue['created_at'], \"%Y-%m-%dT%H:%M:%S\") <= end_date]\n",
    "\n",
    "# Plot commit metrics with bug issues\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, filtered_bug_issues, start_date, end_date)\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, filtered_bug_issues, start_date, end_date, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define label groups and merge 'enhancement' and 'feature request'\n",
    "# label_groups = {'Requests': ['enhancement', 'feature request', 'bug', 'documentation', 'question', 'help wanted']}\n",
    "\n",
    "# # Extract relevant data\n",
    "# def extract_label_data(data, label_groups):\n",
    "#     label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "#     for entry in data:\n",
    "#         created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#         labels = entry['labels']\n",
    "#         for label in labels:\n",
    "#             for group, labels_list in label_groups.items():\n",
    "#                 if label in labels_list:\n",
    "#                     # Merge 'enhancement' and 'feature request' labels\n",
    "#                     if label in ['enhancement', 'feature request']:\n",
    "#                         label = 'enhancement'\n",
    "#                     if label in ['documentation', 'question', 'help wanted']:\n",
    "#                         label = 'support/documentation'\n",
    "#                     label_data[group].append((created_at, label))\n",
    "    \n",
    "#     return label_data\n",
    "\n",
    "# def plot_label_group(data, label_group, group_name, months_per_label=3):\n",
    "#     df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    \n",
    "#     # Convert dates to datetime format\n",
    "#     df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "#     # Resample data by the specified number of months\n",
    "#     resample_rule = f'{months_per_label}M'\n",
    "#     df.set_index('created_at', inplace=True)\n",
    "#     monthly_data = df.groupby([pd.Grouper(freq=resample_rule), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "#     # Plotting the data\n",
    "#     monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "#     # Formatting the plot\n",
    "#     plt.title(f'{group_name} Over Time')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend(title=group_name)\n",
    "\n",
    "#     # Improve the date formatting\n",
    "#     plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "#     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "#     plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "#     # Display the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract label data\n",
    "# label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# # Extract commit data\n",
    "# def extract_commit_data_from_prs(pull_requests_data):\n",
    "#     commit_info = []\n",
    "#     for pr in pull_requests_data:\n",
    "#         for commit in pr['commits']:\n",
    "#             commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#             # Calculate the changes as the sum of additions and deletions\n",
    "#             changes = pr['additions'] + pr['deletions']\n",
    "#             commit_info.append((commit_date, changes))\n",
    "#     return commit_info\n",
    "\n",
    "# commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# # Filter commits by date and size to remove initial outliers\n",
    "# def filter_initial_outliers(commit_info, threshold=0.01):\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     initial_period_end = df_commits['commit_date'].min() + pd.Timedelta(days=30)\n",
    "#     initial_commits = df_commits[df_commits['commit_date'] <= initial_period_end]\n",
    "#     filtered_commits = df_commits[df_commits['commit_date'] > initial_period_end]\n",
    "\n",
    "#     if not initial_commits.empty:\n",
    "#         upper_limit = initial_commits['changes'].quantile(1 - threshold)\n",
    "#         filtered_initial_commits = initial_commits[initial_commits['changes'] <= upper_limit]\n",
    "#         return pd.concat([filtered_initial_commits, filtered_commits])\n",
    "#     return df_commits\n",
    "\n",
    "# filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "\n",
    "# def plot_commit_metrics_with_bugs(commit_info, bug_issues, months_per_label=3, log_scale=False):\n",
    "#     # Convert commit info to DataFrame\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "    \n",
    "#     # Group by the specified number of months\n",
    "#     resample_rule = f'{months_per_label}M'\n",
    "#     monthly_commits = df_commits.groupby([pd.Grouper(freq=resample_rule, key='commit_date')]).size()\n",
    "#     monthly_changes = df_commits.groupby([pd.Grouper(freq=resample_rule, key='commit_date')])['changes'].mean()\n",
    "    \n",
    "#     # Convert bug issues to DataFrame\n",
    "#     df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "#     df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "    \n",
    "#     # Group bug issues by the specified number of months\n",
    "#     monthly_bugs = df_bugs.groupby([pd.Grouper(freq=resample_rule, key='created_at')]).size()\n",
    "    \n",
    "#     fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "#     # Bar plot for monthly commits\n",
    "#     ax1.bar(monthly_commits.index, monthly_commits.values, width=20, alpha=0.6, label='Monthly Commits', color='green')\n",
    "#     ax1.set_xlabel('Date')\n",
    "#     ax1.set_ylabel('Number of Commits', color='green')\n",
    "#     ax1.tick_params(axis='y', labelcolor='green')\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     # Secondary axis for average commit size\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(monthly_changes.index, monthly_changes.values, marker='o', linestyle='-', color='blue', label='Average Commit Size')\n",
    "#     ax2.set_ylabel('Average Commit Size (lines)', color='blue')\n",
    "#     if log_scale: ax2.set_yscale('log')\n",
    "#     ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "#     # Third axis for bug issues\n",
    "#     ax3 = ax1.twinx()\n",
    "#     ax3.spines['right'].set_position(('outward', 60))\n",
    "#     ax3.plot(monthly_bugs.index, monthly_bugs.values, 'r-', marker='x', label='Bug Issues')\n",
    "#     ax3.set_ylabel('Bug Issues', color='red')\n",
    "#     ax3.tick_params(axis='y', labelcolor='red')\n",
    "#     # Set the y-scale for bug issues to display integers only\n",
    "#     ax3.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "#     # Add title and legend\n",
    "#     fig.suptitle('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "#     fig.tight_layout()\n",
    "#     fig.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract the whole time period\n",
    "# start_date = datetime.strptime(issues_data[0]['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "# end_date = datetime.strptime(issues_data[-1]['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# # Filter bug issues\n",
    "# bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "\n",
    "# # Plot commit metrics with bug issues for the whole period, grouped by 3 months (can be changed as needed)\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3)\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define label groups and merge 'enhancement' and 'feature request'\n",
    "# label_groups = {'Requests': ['enhancement', 'feature request', 'bug', 'documentation', 'question', 'help wanted']}\n",
    "\n",
    "# # Extract relevant data\n",
    "# def extract_label_data(data, label_groups):\n",
    "#     label_data = {group: [] for group in label_groups.keys()}\n",
    "    \n",
    "#     for entry in data:\n",
    "#         created_at = datetime.strptime(entry['created_at'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#         labels = entry['labels']\n",
    "#         for label in labels:\n",
    "#             for group, labels_list in label_groups.items():\n",
    "#                 if label in labels_list:\n",
    "#                     # Merge 'enhancement' and 'feature request' labels\n",
    "#                     if label in ['enhancement', 'feature request']:\n",
    "#                         label = 'enhancement'\n",
    "#                     if label in ['documentation', 'question', 'help wanted']:\n",
    "#                         label = 'support/documentation'\n",
    "#                     label_data[group].append((created_at, label))\n",
    "    \n",
    "#     return label_data\n",
    "\n",
    "# def plot_label_group(data, label_group, group_name, months_per_label=3):\n",
    "#     df = pd.DataFrame(data, columns=['created_at', 'label'])\n",
    "    \n",
    "#     # Convert dates to datetime format\n",
    "#     df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "#     # Resample data by month\n",
    "#     df.set_index('created_at', inplace=True)\n",
    "#     monthly_data = df.groupby([pd.Grouper(freq='M'), 'label']).size().unstack(fill_value=0)\n",
    "    \n",
    "#     # Plotting the data\n",
    "#     monthly_data.plot(figsize=(18, 8), marker='o')\n",
    "\n",
    "#     # Formatting the plot\n",
    "#     plt.title(f'{group_name} Over Time')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend(title=group_name)\n",
    "\n",
    "#     # Improve the date formatting\n",
    "#     plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=months_per_label))\n",
    "#     plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "#     plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "#     # Display the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract label data\n",
    "# label_data = extract_label_data(issues_data, label_groups)\n",
    "\n",
    "# # Extract commit data\n",
    "# def extract_commit_data_from_prs(pull_requests_data):\n",
    "#     commit_info = []\n",
    "#     for pr in pull_requests_data:\n",
    "#         for commit in pr['commits']:\n",
    "#             commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "#             # Calculate the changes as the sum of additions and deletions\n",
    "#             changes = pr['additions'] + pr['deletions']\n",
    "#             commit_info.append((commit_date, changes))\n",
    "#     return commit_info\n",
    "\n",
    "# commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# # Filter initial outliers based on the condition provided\n",
    "# def filter_initial_outliers(commit_info):\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     mean_commit_size = df_commits['changes'].mean()\n",
    "    \n",
    "#     # Find the index of the first commit that is smaller than the mean of subsequent commits\n",
    "#     for idx, changes in enumerate(df_commits['changes']):\n",
    "#         if changes < mean_commit_size:\n",
    "#             return df_commits.iloc[idx:]\n",
    "#     return df_commits\n",
    "\n",
    "# filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "\n",
    "# def plot_commit_metrics_with_bugs(commit_info, bug_issues, months_per_label=3, log_scale=False):\n",
    "#     # Convert commit info to DataFrame\n",
    "#     df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "#     df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "\n",
    "#     # Resample data by month\n",
    "#     df_commits.set_index('commit_date', inplace=True)\n",
    "#     df_commits_monthly = df_commits.resample('M').agg({'changes': ['mean', 'count']})\n",
    "#     df_commits_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    \n",
    "#     # Fill NaN values with 0\n",
    "#     df_commits_monthly.fillna(0, inplace=True)\n",
    "\n",
    "#     # Smooth data with rolling average\n",
    "#     df_commits_monthly['monthly_commits'] = df_commits_monthly['monthly_commits'].rolling(window=3, min_periods=1).mean()\n",
    "#     df_commits_monthly['avg_size'] = df_commits_monthly['avg_size'].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "#     # Convert bug issues to DataFrame and resample by month\n",
    "#     df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "#     df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "#     df_bugs.set_index('created_at', inplace=True)\n",
    "#     df_bugs_monthly = df_bugs.resample('M').size()\n",
    "    \n",
    "#     fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "#     # Bar plot for monthly commits\n",
    "#     ax1.bar(df_commits_monthly.index, df_commits_monthly['monthly_commits'], width=20, color='green', alpha=0.6, label='Monthly Commits')\n",
    "#     ax1.set_xlabel('Date')\n",
    "#     ax1.set_ylabel('Number of Commits', color='green')\n",
    "#     ax1.tick_params(axis='y', labelcolor='green')\n",
    "#     ax1.grid(True)\n",
    "\n",
    "#     # Secondary axis for average commit size\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(df_commits_monthly.index, df_commits_monthly['avg_size'], marker='o', linestyle='-', color='blue', label='Average Commit Size')\n",
    "#     ax2.set_ylabel('Average Commit Size (lines)', color='blue')\n",
    "#     if log_scale: ax2.set_yscale('log')\n",
    "#     ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "#     # Third axis for bug issues\n",
    "#     ax3 = ax1.twinx()\n",
    "#     ax3.spines['right'].set_position(('outward', 60))\n",
    "#     ax3.plot(df_bugs_monthly.index, df_bugs_monthly.values, 'r-', marker='x', label='Bug Issues')\n",
    "#     ax3.set_ylabel('Bug Issues', color='red')\n",
    "#     ax3.tick_params(axis='y', labelcolor='red')\n",
    "#     # Set the y-scale for bug issues to display integers only\n",
    "#     ax3.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "#     # Add title and legend\n",
    "#     fig.suptitle('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "#     fig.tight_layout()\n",
    "#     fig.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "#     plt.show()\n",
    "\n",
    "# # Define the time interval for the spike\n",
    "# start_date = datetime(2019, 7, 16)\n",
    "# end_date = datetime(2019, 8, 31)\n",
    "\n",
    "# # Filter commits and bug issues\n",
    "# filtered_commit_info = filter_initial_outliers(commit_info)\n",
    "# bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "\n",
    "# # Plot commit metrics with bug issues\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3)\n",
    "# plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_commits_to_filter_out = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data\n",
    "def extract_commit_data_from_prs(pull_requests_data):\n",
    "    commit_info = []\n",
    "    for pr in pull_requests_data:\n",
    "        for commit in pr['commits']:\n",
    "            commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            # Calculate the changes as the sum of additions and deletions\n",
    "            changes = pr['additions'] + pr['deletions']\n",
    "            commit_info.append((commit_date, changes))\n",
    "    return commit_info\n",
    "\n",
    "commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# Filter out the first number_of_commits_to_filter_out commits\n",
    "def filter_initial_commits(commit_info, n):\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df_commits.sort_values(by='commit_date', inplace=True)\n",
    "    df_filtered = df_commits.iloc[n:]\n",
    "    return df_filtered\n",
    "\n",
    "filtered_commit_info = filter_initial_commits(commit_info, number_of_commits_to_filter_out)\n",
    "\n",
    "def plot_commit_metrics_with_bugs(commit_info, bug_issues, months_per_label=3, log_scale=False):\n",
    "    # Convert commit info to DataFrame\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "    \n",
    "    # Resample data by month\n",
    "    df_commits.set_index('commit_date', inplace=True)\n",
    "    df_commits_monthly = df_commits.resample('M').agg({'changes': ['mean', 'count']})\n",
    "    df_commits_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    df_commits_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Group bug issues by month\n",
    "    df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "    df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "    df_bugs.set_index('created_at', inplace=True)\n",
    "    df_bugs_monthly = df_bugs.resample('M').size()\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    # Bar plot for monthly commits\n",
    "    ax1.bar(df_commits_monthly.index, df_commits_monthly['monthly_commits'], width=20, alpha=0.6, label='Monthly Commits', color='green')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Secondary axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_commits_monthly.index, df_commits_monthly['avg_size'], marker='o', linestyle='-', color='blue', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='blue')\n",
    "    if log_scale: ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Third axis for bug issues\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.plot(df_bugs_monthly.index, df_bugs_monthly.values, 'r-', marker='x', label='Bug Issues')\n",
    "    ax3.set_ylabel('Bug Issues', color='red')\n",
    "    ax3.tick_params(axis='y', labelcolor='red')\n",
    "    # Set the y-scale for bug issues to display integers only\n",
    "    ax3.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Add title and legend\n",
    "    fig.suptitle('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Define bug issues\n",
    "bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "\n",
    "# Plot commit metrics with bug issues\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3)\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract relevant data\n",
    "def extract_commit_data_from_prs(pull_requests_data):\n",
    "    commit_info = []\n",
    "    for pr in pull_requests_data:\n",
    "        for commit in pr['commits']:\n",
    "            commit_date = datetime.strptime(commit['date'], \"%Y-%m-%dT%H:%M:%S\")\n",
    "            # Calculate the changes as the sum of additions and deletions\n",
    "            changes = pr['additions'] + pr['deletions']\n",
    "            commit_info.append((commit_date, changes))\n",
    "    return commit_info\n",
    "\n",
    "commit_info = extract_commit_data_from_prs(pull_requests_data)\n",
    "\n",
    "# Filter out the first number_of_commits_to_filter_out commits\n",
    "def filter_initial_commits(commit_info, n):\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df_commits.sort_values(by='commit_date', inplace=True)\n",
    "    df_filtered = df_commits.iloc[n:]\n",
    "    return df_filtered\n",
    "\n",
    "filtered_commit_info = filter_initial_commits(commit_info, number_of_commits_to_filter_out)\n",
    "\n",
    "def plot_commit_metrics_with_bugs(commit_info, bug_issues, months_per_label=3, log_scale=False, export_format=EXPORT_FORMAT):\n",
    "    # Convert commit info to DataFrame\n",
    "    df_commits = pd.DataFrame(commit_info, columns=['commit_date', 'changes'])\n",
    "    df_commits['commit_date'] = pd.to_datetime(df_commits['commit_date'])\n",
    "    \n",
    "    # Resample data by month\n",
    "    df_commits.set_index('commit_date', inplace=True)\n",
    "    df_commits_monthly = df_commits.resample('M').agg({'changes': ['mean', 'count']})\n",
    "    df_commits_monthly.columns = ['avg_size', 'monthly_commits']\n",
    "    df_commits_monthly.fillna(0, inplace=True)\n",
    "    \n",
    "    # Group bug issues by month\n",
    "    df_bugs = pd.DataFrame(bug_issues, columns=['created_at', 'label'])\n",
    "    df_bugs['created_at'] = pd.to_datetime(df_bugs['created_at'])\n",
    "    df_bugs.set_index('created_at', inplace=True)\n",
    "    df_bugs_monthly = df_bugs.resample('M').size().to_frame(name='bug_issues')\n",
    "\n",
    "    # Export the data for dashboard use\n",
    "    export_df(df_commits_monthly, f'commit_metrics', export_format) \n",
    "    export_df(df_bugs_monthly, f'bug_metrics', export_format)\n",
    "\n",
    "    # Plotting the data (for visualization, if needed)\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    # Bar plot for monthly commits\n",
    "    ax1.bar(df_commits_monthly.index, df_commits_monthly['monthly_commits'], width=20, alpha=0.6, label='Monthly Commits', color='green')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Number of Commits', color='green')\n",
    "    ax1.tick_params(axis='y', labelcolor='green')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Secondary axis for average commit size\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_commits_monthly.index, df_commits_monthly['avg_size'], marker='o', linestyle='-', color='blue', label='Average Commit Size')\n",
    "    ax2.set_ylabel('Average Commit Size (lines)', color='blue')\n",
    "    if log_scale:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Third axis for bug issues\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.plot(df_bugs_monthly.index, df_bugs_monthly['bug_issues'], 'r-', marker='x', label='Bug Issues')\n",
    "    ax3.set_ylabel('Bug Issues', color='red')\n",
    "    ax3.tick_params(axis='y', labelcolor='red')\n",
    "    ax3.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Add title and legend\n",
    "    fig.suptitle('Commit Frequency, Average Commit Size, and Bug Issues Over Time')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "    if EXPORT_PNGS:\n",
    "        if not log_scale: plt.savefig(\"commit_freq__avg_commit_size__bug_issues.png\")\n",
    "        else: plt.savefig(\"commit_freq__avg_commit_size__bug_issues_log.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Define bug issues\n",
    "bug_issues = [issue for issue in issues_data if 'bug' in issue['labels']]\n",
    "\n",
    "# Plot and export commit metrics with bug issues\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3, export_format=EXPORT_FORMAT)\n",
    "plot_commit_metrics_with_bugs(filtered_commit_info, bug_issues, months_per_label=3, log_scale=True, export_format=EXPORT_FORMAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartDelta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
